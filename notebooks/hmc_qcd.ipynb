{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hepmc import *\n",
    "from hepmc.core.densities.qcd import ee_qq_ng, export_hepmc\n",
    "from hepmc.core.densities.sarge import Sarge\n",
    "from hepmc.core.phase_space.rambo import RamboOnDiet\n",
    "from scipy.special import logit, expit\n",
    "from hepmc.core.phase_space.mapping import MappedDensity\n",
    "from hepmc.core.markov.metropolis import DefaultMetropolis\n",
    "from hepmc.core.markov.base import MixingMarkovUpdate\n",
    "from hepmc.core.markov.metropolis_adaptive import AdaptiveMetropolisUpdate\n",
    "from hepmc.core.proposals import Gaussian\n",
    "from hepmc import surrogate\n",
    "from hepmc.core import densities\n",
    "from hepmc.core.hamiltonian.hmc import HamiltonianUpdate\n",
    "from hepmc.core.hamiltonian.spherical_hmc import StaticSphericalHMC\n",
    "from hepmc.core.hamiltonian.wall_hmc import WallHMC\n",
    "from hepmc.core.sampling import Sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeqqg = ee_qq_ng(1, 100., 5., .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo_mapping = RamboOnDiet(100., 3)\n",
    "mapped = MappedDensity(eeqqg, rambo_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate HMC without spherical augmentation\n",
    "start = np.random.random(5)\n",
    "print('start:', start)\n",
    "print('pot:', mapped.pot(start))\n",
    "\n",
    "sampler = HamiltonianUpdate(mapped, densities.gaussian.Gaussian(5, 1.), 30, .001)\n",
    "%time hmc_sample = sampler.sample(15000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmc without spherical mixed with sarge\n",
    "sarge = Sarge(2, 3, 100.)\n",
    "importance = DefaultMetropolis(eeqqg, proposal=sarge)  # using sarge, full momentum sample space\n",
    "local = HamiltonianUpdate(mapped, densities.gaussian.Gaussian(5, 1.), 30, .001)\n",
    "update = MixingMarkovUpdate(5, [importance, local], weights=[.1, .9], target=mapped,\n",
    "                            out_maps={0: rambo_mapping.map_inverse},\n",
    "                            in_maps={0: rambo_mapping.map}) # rambo sample space, important to set target explicitly\n",
    "start = update.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time mc3_sample = update.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = Sample(data=rambo_mapping.map(mc3_sample.data), weights=mc3_sample.weights)\n",
    "export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 5.87 s, total: 20.9 s\n",
      "Wall time: 9.5 s\n"
     ]
    }
   ],
   "source": [
    "nodes = 300\n",
    "nlearn = 100000\n",
    "\n",
    "xvals = np.random.random((nlearn, 5))\n",
    "fvals = mapped.pdf(xvals)\n",
    "\n",
    "# surrogate\n",
    "basis = surrogate.extreme_learning.GaussianBasis(5)\n",
    "#log_vals = -np.ma.log(fvals)\n",
    "#xvals = xvals[~log_vals.mask]\n",
    "#log_vals = log_vals[~log_vals.mask]\n",
    "# train\n",
    "%time params = basis.extreme_learning_train(xvals, fvals, nodes)\n",
    "\n",
    "# surrogate\n",
    "def surrogate_fn(xs):\n",
    "    return basis.eval(*params, xs)[0]\n",
    "\n",
    "# surrogate gradient\n",
    "def surrogate_gradient(xs):\n",
    "    return basis.eval_gradient(*params, xs)\n",
    "#mapped.pot_gradient = surrogate_gradient\n",
    "#mapped.pdf_gradient = surrogate_gradient\n",
    "\n",
    "def pot_gradient(xs):\n",
    "    pdf = mapped.pdf(xs)\n",
    "    if pdf == 0:\n",
    "        return np.full(5, np.inf)\n",
    "    \n",
    "    return -surrogate_gradient(xs) / pdf\n",
    "\n",
    "mapped.pot_gradient = pot_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = np.array([0.30527385, 0.59699739, 0.51985594, 0.57366659, 0.51096818])\n",
    "start = np.random.random(5)\n",
    "sampler = StaticSphericalHMC(mapped, .005, .005, 30, 30)\n",
    "%time hmc_sample = sampler.sample(15000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(hmc_sample, hmc_sample.mean, hmc_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=hmc_sample.weights[5000:])\n",
    "#mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "resamp_idx = np.random.choice(100000, 100000, p=hmc_sample.weights[50000:]/hmc_sample.weights[50000:].sum())\n",
    "hmc_resample = Sample(data=hmc_sample.data[50000:][resamp_idx], weights=np.full(100000, 1./100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_resample.data), weights=hmc_resample.weights)\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hmc_sample.weights.min())\n",
    "print(hmc_sample.weights.max())\n",
    "print(hmc_sample.weights.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmc mixed with sarge\n",
    "sarge = Sarge(2, 3, 100.)\n",
    "importance = DefaultMetropolis(eeqqg, proposal=sarge)  # using sarge, full momentum sample space\n",
    "local = StaticSphericalHMC(mapped, .1, .1, 20, 20)  # using rambo, rambo sample space\n",
    "update = MixingMarkovUpdate(5, [importance, local], weights=[.1, .9], target=mapped,\n",
    "                            out_maps={0: rambo_mapping.map_inverse},\n",
    "                            in_maps={0: rambo_mapping.map}) # rambo sample space, important to set target explicitly\n",
    "start = update.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time mc3_sample = update.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = Sample(data=rambo_mapping.map(mc3_sample.data), weights=mc3_sample.weights)\n",
    "export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.partition(mc3_sample.weights, -100)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argpartition(mc3_sample.weights, -100)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "resamp_idx = np.random.choice(100000, 100000, p=mc3_sample.weights)\n",
    "mc3_resample = MarkovSample(data=mc3_sample.data[resamp_idx], weights=np.full(100000, 1./100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = MarkovSample(data=rambo_mapping.map(mc3_resample.data), weights=mc3_resample.weights)\n",
    "densities.export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc3_sample_thinned = MarkovSample(data=mc3_sample.data[::10], weights=mc3_sample.weights[::10])\n",
    "mc3_sample_thinned.weights /= mc3_sample_thinned.weights.sum()\n",
    "# resample\n",
    "resamp_idx = np.random.choice(10000, 10000, p=mc3_sample_thinned.weights)\n",
    "mc3_resample = MarkovSample(data=mc3_sample_thinned.data[resamp_idx], weights=np.full(10000, 1./10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = MarkovSample(data=rambo_mapping.map(mc3_resample.data), weights=mc3_resample.weights)\n",
    "densities.export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis with Rambo mapping\n",
    "met = DefaultMetropolis(mapped, Gaussian(5, .01))\n",
    "start = met.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time met_sample = met.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_met_sample = Sample(data=rambo_mapping.map(met_sample.data), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_met_sample, \"../samples/qcd/2-3/metropolis.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: [0.27038678 0.19230907 0.08674884 0.76285181 0.61187865]\n",
      "pdf: [3272.80076587]\n",
      "pot: [10.7473158]\n",
      "pot_grad: [[ 0.10082226  0.01883149  0.04319678  0.01256269 -0.05186166]]\n",
      "Event 1\t(batch acceptance rate: 0.000000)\n",
      "Event 2\t(batch acceptance rate: 0.000000)\tmax sequence length: 3\n",
      "Event 3\t(batch acceptance rate: 0.000000)\tmax sequence length: 4\n",
      "Event 4\t(batch acceptance rate: 0.000000)\tmax sequence length: 5\n",
      "Event 5\t(batch acceptance rate: 0.200000)\tmax sequence length: 5\n",
      "Event 6\t(batch acceptance rate: 0.333333)\tmax sequence length: 5\n",
      "Event 7\t(batch acceptance rate: 0.428571)\tmax sequence length: 5\n",
      "Event 8\t(batch acceptance rate: 0.500000)\tmax sequence length: 5\n",
      "Event 9\t(batch acceptance rate: 0.555556)\tmax sequence length: 5\n",
      "Event 10\t(batch acceptance rate: 0.600000)\tmax sequence length: 5\n",
      "Event 20\t(batch acceptance rate: 0.800000)\tmax sequence length: 5\n",
      "Event 30\t(batch acceptance rate: 0.866667)\tmax sequence length: 5\n",
      "Event 40\t(batch acceptance rate: 0.900000)\tmax sequence length: 5\n",
      "Event 50\t(batch acceptance rate: 0.920000)\tmax sequence length: 5\n",
      "Event 60\t(batch acceptance rate: 0.933333)\tmax sequence length: 5\n",
      "Event 70\t(batch acceptance rate: 0.942857)\tmax sequence length: 5\n",
      "Event 80\t(batch acceptance rate: 0.950000)\tmax sequence length: 5\n",
      "Event 90\t(batch acceptance rate: 0.955556)\tmax sequence length: 5\n",
      "Event 100\t(batch acceptance rate: 0.960000)\tmax sequence length: 5\n",
      "Event 200\t(batch acceptance rate: 0.980000)\tmax sequence length: 5\n",
      "Event 300\t(batch acceptance rate: 0.986667)\tmax sequence length: 5\n",
      "Event 400\t(batch acceptance rate: 0.990000)\tmax sequence length: 5\n",
      "Event 500\t(batch acceptance rate: 0.992000)\tmax sequence length: 5\n",
      "Event 600\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 700\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 800\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 900\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 1000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 2000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 3000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 4000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 5000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 6000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 7000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 8000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "Event 9000\t(batch acceptance rate: 1.000000)\tmax sequence length: 5\n",
      "start: [0.06841656 0.27231614 0.55679742 0.13517055 0.87967828]\n",
      "Event 1\t(batch acceptance rate: 0.000000)\n",
      "Event 2\t(batch acceptance rate: 0.000000)\tmax sequence length: 3\n",
      "Event 3\t(batch acceptance rate: 0.000000)\tmax sequence length: 4\n",
      "Event 4\t(batch acceptance rate: 0.000000)\tmax sequence length: 5\n",
      "Event 5\t(batch acceptance rate: 0.000000)\tmax sequence length: 6\n",
      "Event 6\t(batch acceptance rate: 0.000000)\tmax sequence length: 7\n",
      "Event 7\t(batch acceptance rate: 0.000000)\tmax sequence length: 8\n",
      "Event 8\t(batch acceptance rate: 0.000000)\tmax sequence length: 9\n",
      "Event 9\t(batch acceptance rate: 0.000000)\tmax sequence length: 10\n",
      "Event 10\t(batch acceptance rate: 0.000000)\tmax sequence length: 11\n",
      "Event 20\t(batch acceptance rate: 0.000000)\tmax sequence length: 21\n",
      "Event 30\t(batch acceptance rate: 0.000000)\tmax sequence length: 31\n",
      "Event 40\t(batch acceptance rate: 0.000000)\tmax sequence length: 41\n",
      "Event 50\t(batch acceptance rate: 0.060000)\tmax sequence length: 48\n",
      "Event 60\t(batch acceptance rate: 0.216667)\tmax sequence length: 48\n",
      "Event 70\t(batch acceptance rate: 0.328571)\tmax sequence length: 48\n",
      "Event 80\t(batch acceptance rate: 0.412500)\tmax sequence length: 48\n",
      "Event 90\t(batch acceptance rate: 0.477778)\tmax sequence length: 48\n",
      "Event 100\t(batch acceptance rate: 0.530000)\tmax sequence length: 48\n",
      "Event 200\t(batch acceptance rate: 0.765000)\tmax sequence length: 48\n",
      "Event 300\t(batch acceptance rate: 0.843333)\tmax sequence length: 48\n",
      "Event 400\t(batch acceptance rate: 0.882500)\tmax sequence length: 48\n",
      "Event 500\t(batch acceptance rate: 0.906000)\tmax sequence length: 48\n",
      "Event 600\t(batch acceptance rate: 0.921667)\tmax sequence length: 48\n",
      "Event 700\t(batch acceptance rate: 0.932857)\tmax sequence length: 48\n",
      "Event 800\t(batch acceptance rate: 0.941250)\tmax sequence length: 48\n",
      "Event 900\t(batch acceptance rate: 0.947778)\tmax sequence length: 48\n",
      "Event 1000\t(batch acceptance rate: 0.953000)\tmax sequence length: 48\n",
      "Event 2000\t(batch acceptance rate: 0.976500)\tmax sequence length: 48\n",
      "Event 3000\t(batch acceptance rate: 0.984333)\tmax sequence length: 48\n",
      "Event 4000\t(batch acceptance rate: 0.988250)\tmax sequence length: 48\n",
      "Event 5000\t(batch acceptance rate: 0.990600)\tmax sequence length: 48\n",
      "Event 6000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 7000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 8000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 9000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 10000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 20000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 30000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 40000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 50000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 60000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 70000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 80000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "Event 90000\t(batch acceptance rate: 1.000000)\tmax sequence length: 48\n",
      "CPU times: user 7min 55s, sys: 57.5 ms, total: 7min 55s\n",
      "Wall time: 7min 56s\n"
     ]
    }
   ],
   "source": [
    "# Wall HMC\n",
    "#start = np.random.random(5)\n",
    "#start = np.array([.96, .4, .6, .5, .5])\n",
    "start = np.array([0.27038678, 0.19230907, 0.08674884, 0.76285181, 0.61187865])\n",
    "print('start:', start)\n",
    "print('pdf:', mapped.pdf(start))\n",
    "print('pot:', mapped.pot(start))\n",
    "print('pot_grad:', mapped.pot_gradient(start))\n",
    "sampler = WallHMC(mapped, .1, .1, 10, 10)\n",
    "#sampler.p_dist.cov = np.diag(5*[.01])\n",
    "start = sampler.sample(10000, start).data[-1] # warmup\n",
    "print('start:', start)\n",
    "%time wallhmc_sample = sampler.sample(100000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallhmc_sample_df = pd.DataFrame(wallhmc_sample.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sns.PairGrid(wallhmc_sample_df)\n",
    "k = k.map_diag(plt.hist, bins=15)\n",
    "k = k.map_offdiag(plt.hist2d, bins=15, cmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_wallhmc_sample = Sample(data=rambo_mapping.map(wallhmc_sample.data), weights=np.full(100000, 1/100000))\n",
    "export_hepmc(100., mapped_wallhmc_sample, \"../samples/qcd/2-3/wallhmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hepmc",
   "language": "python",
   "name": "hepmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
