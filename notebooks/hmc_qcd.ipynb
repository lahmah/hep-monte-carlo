{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hepmc import *\n",
    "from hepmc.core.densities.qcd import ee_qq_ng, export_hepmc\n",
    "from hepmc.core.densities.sarge import Sarge\n",
    "from hepmc.core.phase_space.rambo import RamboOnDiet\n",
    "from scipy.special import logit, expit\n",
    "from hepmc.core.phase_space.mapping import MappedDensity\n",
    "from hepmc.core.markov.metropolis import DefaultMetropolis\n",
    "from hepmc.core.markov.base import MixingMarkovUpdate\n",
    "from hepmc.core.markov.metropolis_adaptive import AdaptiveMetropolisUpdate\n",
    "from hepmc.core.proposals import Gaussian\n",
    "from hepmc import surrogate\n",
    "from hepmc.core import densities\n",
    "from hepmc.core.hamiltonian.hmc import HamiltonianUpdate\n",
    "from hepmc.core.hamiltonian.spherical_hmc import StaticSphericalHMC\n",
    "from hepmc.core.hamiltonian.wall_hmc import WallHMC\n",
    "from hepmc.core.sampling import Sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeqqg = ee_qq_ng(1, 100., 5., .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo_mapping = RamboOnDiet(100., 3)\n",
    "mapped = MappedDensity(eeqqg, rambo_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate HMC without spherical augmentation\n",
    "start = np.random.random(5)\n",
    "print('start:', start)\n",
    "print('pot:', mapped.pot(start))\n",
    "\n",
    "sampler = HamiltonianUpdate(mapped, densities.gaussian.Gaussian(5, 1.), 30, .001)\n",
    "%time hmc_sample = sampler.sample(15000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmc without spherical mixed with sarge\n",
    "sarge = Sarge(2, 3, 100.)\n",
    "importance = DefaultMetropolis(eeqqg, proposal=sarge)  # using sarge, full momentum sample space\n",
    "local = HamiltonianUpdate(mapped, densities.gaussian.Gaussian(5, 1.), 30, .001)\n",
    "update = MixingMarkovUpdate(5, [importance, local], weights=[.1, .9], target=mapped,\n",
    "                            out_maps={0: rambo_mapping.map_inverse},\n",
    "                            in_maps={0: rambo_mapping.map}) # rambo sample space, important to set target explicitly\n",
    "start = update.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time mc3_sample = update.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = Sample(data=rambo_mapping.map(mc3_sample.data), weights=mc3_sample.weights)\n",
    "export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 300\n",
    "nlearn = 100000\n",
    "\n",
    "xvals = np.random.random((nlearn, 5))\n",
    "fvals = mapped.pdf(xvals)\n",
    "\n",
    "# surrogate\n",
    "basis = surrogate.extreme_learning.GaussianBasis(5)\n",
    "#log_vals = -np.ma.log(fvals)\n",
    "#xvals = xvals[~log_vals.mask]\n",
    "#log_vals = log_vals[~log_vals.mask]\n",
    "# train\n",
    "%time params = basis.extreme_learning_train(xvals, fvals, nodes)\n",
    "\n",
    "# surrogate\n",
    "def surrogate_fn(xs):\n",
    "    return basis.eval(*params, xs)[0]\n",
    "\n",
    "# surrogate gradient\n",
    "def surrogate_gradient(xs):\n",
    "    return basis.eval_gradient(*params, xs)\n",
    "#mapped.pot_gradient = surrogate_gradient\n",
    "#mapped.pdf_gradient = surrogate_gradient\n",
    "\n",
    "def pot_gradient(xs):\n",
    "    pdf = mapped.pdf(xs)\n",
    "    if pdf == 0:\n",
    "        return np.full(5, np.inf)\n",
    "    \n",
    "    return -surrogate_gradient(xs) / pdf\n",
    "\n",
    "mapped.pot_gradient = pot_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = np.array([0.30527385, 0.59699739, 0.51985594, 0.57366659, 0.51096818])\n",
    "start = np.random.random(5)\n",
    "sampler = StaticSphericalHMC(mapped, .005, .005, 30, 30)\n",
    "%time hmc_sample = sampler.sample(15000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(hmc_sample, hmc_sample.mean, hmc_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=hmc_sample.weights[5000:])\n",
    "#mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_sample.data[5000:]), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "resamp_idx = np.random.choice(100000, 100000, p=hmc_sample.weights[50000:]/hmc_sample.weights[50000:].sum())\n",
    "hmc_resample = Sample(data=hmc_sample.data[50000:][resamp_idx], weights=np.full(100000, 1./100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_hmc_sample = Sample(data=rambo_mapping.map(hmc_resample.data), weights=hmc_resample.weights)\n",
    "export_hepmc(100., mapped_hmc_sample, \"../samples/qcd/2-3/hmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hmc_sample.weights.min())\n",
    "print(hmc_sample.weights.max())\n",
    "print(hmc_sample.weights.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmc mixed with sarge\n",
    "sarge = Sarge(2, 3, 100.)\n",
    "importance = DefaultMetropolis(eeqqg, proposal=sarge)  # using sarge, full momentum sample space\n",
    "local = StaticSphericalHMC(mapped, .1, .1, 20, 20)  # using rambo, rambo sample space\n",
    "update = MixingMarkovUpdate(5, [importance, local], weights=[.1, .9], target=mapped,\n",
    "                            out_maps={0: rambo_mapping.map_inverse},\n",
    "                            in_maps={0: rambo_mapping.map}) # rambo sample space, important to set target explicitly\n",
    "start = update.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time mc3_sample = update.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = Sample(data=rambo_mapping.map(mc3_sample.data), weights=mc3_sample.weights)\n",
    "export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.partition(mc3_sample.weights, -100)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argpartition(mc3_sample.weights, -100)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "resamp_idx = np.random.choice(100000, 100000, p=mc3_sample.weights)\n",
    "mc3_resample = MarkovSample(data=mc3_sample.data[resamp_idx], weights=np.full(100000, 1./100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = MarkovSample(data=rambo_mapping.map(mc3_resample.data), weights=mc3_resample.weights)\n",
    "densities.export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc3_sample_thinned = MarkovSample(data=mc3_sample.data[::10], weights=mc3_sample.weights[::10])\n",
    "mc3_sample_thinned.weights /= mc3_sample_thinned.weights.sum()\n",
    "# resample\n",
    "resamp_idx = np.random.choice(10000, 10000, p=mc3_sample_thinned.weights)\n",
    "mc3_resample = MarkovSample(data=mc3_sample_thinned.data[resamp_idx], weights=np.full(10000, 1./10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_mc3_sample = MarkovSample(data=rambo_mapping.map(mc3_resample.data), weights=mc3_resample.weights)\n",
    "densities.export_hepmc(100., mapped_mc3_sample, \"../samples/qcd/2-3/hmc_sarge.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis with Rambo mapping\n",
    "met = DefaultMetropolis(mapped, Gaussian(5, .01))\n",
    "start = met.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time met_sample = met.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_met_sample = Sample(data=rambo_mapping.map(met_sample.data), weights=np.full(10000, 1/10000))\n",
    "export_hepmc(100., mapped_met_sample, \"../samples/qcd/2-3/metropolis.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall HMC\n",
    "#start = np.random.random(5)\n",
    "#start = np.array([.96, .4, .6, .5, .5])\n",
    "start = np.array([0.27038678, 0.19230907, 0.08674884, 0.76285181, 0.61187865])\n",
    "print('start:', start)\n",
    "print('pdf:', mapped.pdf(start))\n",
    "print('pot:', mapped.pot(start))\n",
    "print('pot_grad:', mapped.pot_gradient(start))\n",
    "sampler = WallHMC(mapped, .1, .1, 10, 10)\n",
    "#sampler.p_dist.cov = np.diag(5*[.01])\n",
    "start = sampler.sample(10000, start).data[-1] # warmup\n",
    "print('start:', start)\n",
    "%time wallhmc_sample = sampler.sample(100000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallhmc_sample_df = pd.DataFrame(wallhmc_sample.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sns.PairGrid(wallhmc_sample_df)\n",
    "k = k.map_diag(plt.hist, bins=15)\n",
    "k = k.map_offdiag(plt.hist2d, bins=15, cmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_wallhmc_sample = Sample(data=rambo_mapping.map(wallhmc_sample.data), weights=np.full(100000, 1/100000))\n",
    "export_hepmc(100., mapped_wallhmc_sample, \"../samples/qcd/2-3/wallhmc.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
