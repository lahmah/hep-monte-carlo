{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hepmc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Integration (Ordinary MC)\n",
    "Estimate multi-dimensional integral (over unit hypercube) by randomly sampling $N$ abscissas: \n",
    "$$ I = \\int{d^dx \\; f(x)} \\approx  E = \\frac{1}{N} \\sum_{n=1}^{N}{f(x_n)} , $$\n",
    "with equality for $N \\rightarrow \\infty $ (if the random process used to sample $x_n$ has a constant natural density). Define variance of a function $f$ as\n",
    "$$ \\sigma^2(f) = \\int { d^dx \\; (f(x)-I)^2 } .$$\n",
    "Using this, the variance of the Monte Carlo estimate is\n",
    "$$ \n",
    "\\int { dx_1...dx_N \\left( \\frac{1}{N} \\sum_{n=1}^N f(x_n)- I \\right)^2 }\n",
    "= \\int { dx_1...dx_N \\frac{1}{N^2} \\sum_{n=1}^N \\sum_{m=1}^N (f(x_n)- I)(f(x_m)- I) } \n",
    "= \\int {dx_1...dx_N \\frac{1}{N^2} \\sum_n{(f(x_n)-I)^2}}\n",
    "= \\frac{\\sigma(f)^2}{N},\n",
    "$$\n",
    "since for $n \\neq m$ the integrals separate and clearly $\\int{dx\\;(f(x)-I)} = 0$. \n",
    "\n",
    "The variance can be estimated using the unbiased sample variance\n",
    "$$ S^2 = \\frac{1}{N-1} \\sum_{n=1}^N{(f(x_n)-E)^2} = \\frac{1}{N-1}\\sum_n{f(x_n)^2 } - \\frac{N}{N-1}E^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and introduction to visualizations\n",
    "There are several methods for comparing the integration results from one or multiple Monte Carlo methods.\n",
    "For a first example, note that the error aproximation from above (and the following Monte Carlo variants) relies on \n",
    "square-integrability of the integrand. If this is not given, the error estimate is not reliable and the estimates might not even\n",
    "follow a Gaussian as predicted (in the limit) by the central limit theorem.\n",
    "\n",
    "To exapnd on this, let us consider one square-integrable $\\sin{(2 \\pi x)}$ and one not square-integrable function $\\sqrt{1/x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the implementation\n",
    "Generally, n-dimensional functions are expected to take n numpy arrays (of equal lengths) as arguments.\n",
    "If the function factorizes, this makes it very easy to implement:\n",
    "```python\n",
    "def fn(x, y, z):\n",
    "    return x * y * z\n",
    "```\n",
    "\n",
    "However there might be cases in which it doesn't, in which case `np.vectorize` can be used:\n",
    "```python\n",
    "@np.vectorize\n",
    "def fn(x, y):\n",
    "    # granted, this is rediculous, but it serves as an example.\n",
    "    if str(x).count('0') > 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return x + y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodic sin in 1 dimension\n",
    "sin_1d = lambda x: np.sin(2*np.pi*x)\n",
    "# periodic sin in any dimension; true integral value is 0\n",
    "sin_nd = lambda *x: np.prod([sin_1d(xi) for xi in x], axis=0)\n",
    "\n",
    "sqrtx_inv = lambda *x: np.sqrt(1/np.prod(x, axis=0))\n",
    "sqrtx_inv_trueval = lambda dim: 2**dim  # true integral value depends on dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive comparison of estimates\n",
    "While not providing very statistically rigorous insights, a simple plot of the estimates with predicted error bars over \n",
    "a range of sample sizes (i.e. number of function iterations) can give an intuitive understanding of how the method behaves.\n",
    "\n",
    "#### Number of iterations, sample size, number of function evaluations\n",
    "When calling the plain Monte Carlo method, a number $N$ has to be given. This is the total sample size, i.e. the total number\n",
    "of times the function is evaluated to estimate the integral. This is straight forward for the plain MC method, but later MC\n",
    "variants have internal iterations (loops) with possibly varying number of sample sizes, and the term *number of iterations* becomes\n",
    "ambiguous. The key parameter to compare the efficiency of different methods is the number of function evaluations.\n",
    "To make this comparison easier, the various MC methods provide interfaces to fix internal parameters and then compute the integral\n",
    "for a given number $N$ of function evaluations.\n",
    "\n",
    "The following functions plot results over a range of function evaluations. Thus, the methods provided must only take two arguments,\n",
    "the function and a number of function evaluations. For plain Monte Carlo this is just the object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimates(f, true_value, methods=[], Ns=range(100, 1100, 50)):\n",
    "    \"\"\" Plot estimates of the integral over a range of sample sizes with predicted errorbars. \n",
    "    \n",
    "    If method has attribute method_name use this as label.\n",
    "    \"\"\"\n",
    "    if len(methods) == 0:\n",
    "        raise ValueError(\"Need to provide at least one method.\")\n",
    "        \n",
    "    Ns = np.array(Ns)\n",
    "    count = len(Ns)\n",
    "    \n",
    "    estimates = np.empty(count)\n",
    "    errors = np.empty(count)\n",
    "    for m, method in zip(range(len(methods)), methods):\n",
    "        for i, N in zip(range(count), Ns):\n",
    "            sample = method(f, N)\n",
    "            estimates[i], errors[i] = sample.integral, sample.integral_err\n",
    "            \n",
    "        name = method.method_name if hasattr(method, 'method_name') else \"Method %d\"%m\n",
    "        plt.errorbar(Ns, estimates, yerr=errors, fmt='.', label=name)\n",
    "    \n",
    "    plt.plot([min(Ns), max(Ns)], [true_value, true_value], \"-\", label=\"true value\")\n",
    "    # configure plot\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot estimates for sine and 1/sqrt(x) in any dimension\n",
    "dim = 10\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"%d dimensional sine\"%dim)\n",
    "plot_estimates(sin_nd, 0, Ns=range(10, 500, 10), methods=[PlainMC(dim)])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"%d dimensional 1/sqrt(x)\"%dim)\n",
    "plot_estimates(sqrtx_inv, sqrtx_inv_trueval(dim), Ns=range(10, 500, 10), methods=[PlainMC(dim)])\n",
    "plt.show()\n",
    "\n",
    "# Though qunatitatively, the 1/sqrt(x) integration can already be seen to behave oddly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviour of the error\n",
    "While useful to check if the method works, the plot of estimates does not show quantitatively how the predicted error behaves,\n",
    "especially in comparison to the actual deviation.\n",
    "\n",
    "The following function plots the root mean squared of actual deviations over a given number of `iterations` for each given number of\n",
    "function evaluations on a log-log scale. The MC methods are meant to (statistically) estimate this deviation (i.e. variance) \n",
    "by the returned error. In addition to the RMS devations, the (square root of) mean predicted variances are plotted.\n",
    "\n",
    "If desired (`plot_fit=True`), the slope in the double-logarithic plot is fitted and displayed.\n",
    "\n",
    "#### Expected behaviour\n",
    "According to the analytical treatment at the beginning, the statistical error is expected to scale like $\\sqrt{1/N}$. In the\n",
    "double-logarithmic plot this corresponds to a line with slope $-1/2$. Values smaller than this (more negative) would imply faster\n",
    "convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rms(f, true_value, Ns=range(10, 1000, 100), iterations=1, methods=[], plot_fit=False):\n",
    "    \"\"\" Plot RMS of deviations and RM of predicted variances. \"\"\"\n",
    "    if len(methods) == 0:\n",
    "        raise ValueError(\"Need to provide at least one method.\")\n",
    "        \n",
    "    Ns = np.array(Ns)\n",
    "    # temporarily store the results of the methods in each iteration\n",
    "    iteration_errors = np.empty(iterations)\n",
    "    iteration_estimates = np.empty(iterations)\n",
    "    \n",
    "    for m, method in zip(range(len(methods)), methods):\n",
    "        # store results for each sample size in Ns\n",
    "        mean_vars = np.empty(len(Ns))\n",
    "        rms = np.empty(len(Ns))\n",
    "        \n",
    "        for i, N in zip(range(len(Ns)), Ns):\n",
    "            for j in range(iterations):\n",
    "                sample = method(f, N)\n",
    "                iteration_estimates[j], iteration_errors[j] = sample.integral, sample.integral_err\n",
    "            # compute means\n",
    "            mean_vars[i] = np.mean(iteration_errors**2)\n",
    "            rms[i] = np.mean((iteration_estimates-true_value)**2)\n",
    "\n",
    "        sqrt_mean_vars = np.sqrt(mean_vars)\n",
    "        rms = np.sqrt(rms)\n",
    "        \n",
    "        if plot_fit:\n",
    "            fit = np.polyfit(np.log(Ns), np.log(rms), 1)\n",
    "            plt.loglog(Ns, np.exp(np.poly1d(fit)(np.log(Ns))), label=\"Fit, power %.2f\"%fit[0])\n",
    "\n",
    "        name = method.method_name if hasattr(method, 'method_name') else \"Method %d\"%i\n",
    "        plt.loglog(Ns, sqrt_mean_vars, \"o\", label=\"predicted (%s)\"%name, color=\"C%d\"%m)\n",
    "        plt.loglog(Ns, rms, \"x\", label=\"RMS deviations\", color=\"C%d\"%m)\n",
    "        \n",
    "    plt.xlabel(\"N\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, which=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors for sine and 1/sqrt(x) in any dimension\n",
    "dim = 10\n",
    "it = 100  # number of iterations over which to average\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"%d dimensional sine\"%dim)\n",
    "plot_rms(sin_nd, 0, Ns=range(10, 500, 10), iterations=it, methods=[PlainMC(dim)], plot_fit=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"%d dimensional 1/sqrt(x)\"%dim)\n",
    "plot_rms(sqrtx_inv, sqrtx_inv_trueval(dim), Ns=range(10, 500, 10), iterations=it, methods=[PlainMC(dim)], plot_fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate distribution\n",
    "The estimates are predicted distribute according to a Gaussion with predicted standard deviation.\n",
    "The following funtion plots the distribution of estimates of a method over a number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(fn, true_val, N, iterations, method, bins=20, name=\"\", alpha=1):\n",
    "    \"\"\" Plot the distribution of estimates and the predicted Gaussian.\n",
    "    \n",
    "    For the variance of the Gaussian, use the mean of variances obtained from the\n",
    "    returned values of the method.\n",
    "    \n",
    "    Args:\n",
    "        N: Number of function evaluatios to be passed to method.\n",
    "        iterations: Number of estimates to compute with method, choose bins accordingly.\n",
    "        bins: Number of bins for the histogram of computed estimates.\n",
    "        alpha: Plot histogram with alpha value, useful if comparing multiple distributions.\n",
    "        name: Use in label for the plot, useful if comparing multiple distributions.\n",
    "    \"\"\"\n",
    "    values = np.empty(iterations)\n",
    "    mean_var = 0\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        sample =  method(fn, N)\n",
    "        values[i], var = sample.integral, sample.integral_err\n",
    "        mean_var += var**2 / iterations\n",
    "        \n",
    "    plt.hist(values, bins=bins, normed=True, label=\"Actual \" + name, alpha=alpha)\n",
    "    \n",
    "    x = np.linspace(true_val-4*np.sqrt(mean_var), true_val+4*np.sqrt(mean_var), 1000)\n",
    "    plt.plot(x, np.exp(-(x-true_val)**2/2/mean_var) / np.sqrt(np.pi*2*mean_var), label=\"Predicted \" + name)\n",
    "    \n",
    "    # config\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors for sine and 1/sqrt(x) in any dimension\n",
    "dim = 5\n",
    "it = 1000  # number of iterations for the histogram\n",
    "bins = 50  # number of bins for the histograms\n",
    "N = 1000   # number of function evaluations\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"%d dimensional sine\"%dim)\n",
    "plot_distribution(sin_nd, 0, N=N, iterations=it, method=PlainMC(dim), bins=bins)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"%d dimensional 1/sqrt(x)\"%dim)\n",
    "plot_distribution(sqrtx_inv, sqrtx_inv_trueval(dim), N=N, iterations=it, method=PlainMC(dim), bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Reducing Techniques\n",
    "The error in the MC estimate scales like $1/\\sqrt{N}$, which constitutes a slow convergence. Improving this value is desirable.\n",
    "### Stratified sampling\n",
    "Split $M=[0,1]^d$ into $k$ regions $M_j$, in each region perform MC with $N_j$ points. We obtain the estimate\n",
    "$$ E = \\sum_{j=1}^k {\\frac{\\text{vol}(M_j)}{N_j}} \\sum_{n=1}^{N_j} {f(x_{jn})}, $$\n",
    "and the variance of the MC estimate becomes\n",
    "$$ \n",
    "\\sum_{j=1}^k {\\frac{\\text{vol}(M_j)^2}{N_j} \\left.\\sigma^2(f)\\right|_{M_j} }, \\; \\text{with} \\;  \n",
    "\\left.\\sigma^2(f)\\right|_{M_j} = \\frac{1}{\\text{vol}(M_j)}\\int_{M_j} {dx \\left( f(x) - \\frac{1}{\\text{vol}(M_j)}\\int_{M_j}{dx f(x)}  \\right)^2}.\n",
    "$$\n",
    "Whether this method reduces or increases the variance depends on the choice of subspaces and respective point counts. It is obvious from the expression that to reduce the error/variance, regions with larger variance should have assigned larger $N_j$. Generally, for minimal variance the number of points must be chosen proportional to $\\left.\\sigma(f)\\right|_{M_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumes\n",
    "At the core of stratified sampling (as well as the VEGAS algorithm) is a subdivision of the volume, with optionally\n",
    "different weights / sample sizes in each region.\n",
    "\n",
    "`GridVolumes` provides a simple grid-like (divisions of arbitrary size in each dimension) division of volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate and sample a 2D volume, divided into 4 regions (2 in each dim), with doulbe weight on the second region\n",
    "volumes = GridVolumes(ndim=2, default_base_count=1, base_counts={(0,1):2}, divisions=2)\n",
    "\n",
    "for N, sample, vol in volumes.iterate(1):\n",
    "    print(N, vol)\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: uniform grid with custom weights\n",
    "vols = GridVolumes(divisions=4, ndim=1, default_base_count=2, base_counts={(1,): 1, (3,):3})\n",
    "vols.plot_pdf()\n",
    "xs = np.linspace(0, 1, 1000)\n",
    "plt.plot(xs, vols.pdf(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = vols.random_bins(3)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols.pdf_indices(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: custom widths\n",
    "vols = GridVolumes(bounds=([0,.1,.4,.6,1],), base_counts={(3,):1}, default_base_count=2)\n",
    "vols.plot_pdf()\n",
    "xs = np.linspace(0, 1, 1000)\n",
    "plt.plot(xs, vols.pdf(xs))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Plotting the resulting probability distribution might be misleading for the stratified sampling method.\n",
    "For stratified sampling each subregion is sampled independantly with deterministic sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example consider a function that oscillates rapidly in one half of the volume (interval).\n",
    "# Stratified sampling is more efficient than plain MC if sampling more in this region.\n",
    "\n",
    "half_sin = lambda x: (x<.5)*(1+np.sin(4*10*np.pi*x)) + (x>.5)*2\n",
    "half_sin_integral = 1.5\n",
    "\n",
    "volumes = GridVolumes(divisions=2, default_base_count=1, base_counts={(0,):50})\n",
    "mc_strat = StratifiedMC(volumes)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(221)\n",
    "\n",
    "# plot the function\n",
    "plt.title(\"Function\")\n",
    "volumes.plot_pdf()\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x,half_sin(x))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "# compare to plain MC\n",
    "plot_estimates(half_sin, half_sin_integral, Ns=range(volumes.total_base_count, volumes.total_base_count*100, volumes.total_base_count), \n",
    "               methods=[PlainMC(1), mc_strat.get_interface_infer_multiple()])\n",
    "\n",
    "plt.subplot(223)\n",
    "# compare errors\n",
    "plot_rms(half_sin, half_sin_integral, Ns=range(volumes.total_base_count, volumes.total_base_count*100, volumes.total_base_count*5), iterations=100,\n",
    "         methods=[PlainMC(1), mc_strat.get_interface_infer_multiple()])\n",
    "\n",
    "plt.subplot(224)\n",
    "# compare distributions\n",
    "plot_distribution(half_sin, half_sin_integral, volumes.total_base_count*100, 1000, PlainMC(1), name=\"plain\", alpha=.6)\n",
    "plot_distribution(half_sin, half_sin_integral, volumes.total_base_count*100, 1000, mc_strat.get_interface_infer_multiple(), name=\"strat\", alpha=.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complicated test function\n",
    "tf_a = 16\n",
    "tf_b = 1/.2**2\n",
    "def osc(x):\n",
    "    return x**2*np.cos(tf_a*np.pi*x/.2)**2*tf_b\n",
    "def bound(a, x, b):\n",
    "    return (x>a)*(x<=b)\n",
    "def test_function(x):\n",
    "    return bound(.2, x, .4)*osc(x-.2) + bound(.4, x, .6) + bound(.6, x, .8)*osc(.8-x)\n",
    "test_function_true_value = 2*.0333531 + .2\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x,test_function(x))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different weight choices: Areas with high variance vs high value.\n",
    "# For stratified sampling it is best to prioratize areas with high variance (not value).\n",
    "# This is substantially different from importance sampling.\n",
    "\n",
    "# plot function and volume choices for stratified sampling\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Function\")\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x,test_function(x))\n",
    "plt.grid()\n",
    "\n",
    "vol1 = GridVolumes(divisions=5, default_base_count=14)\n",
    "vol2 = GridVolumes(divisions=5, default_base_count=2, base_counts={(1,):22, (3,):22, (2,):22})\n",
    "vol3 = GridVolumes(divisions=5, default_base_count=2, base_counts={(1,):32, (3,):32})\n",
    "vol1.plot_pdf(\"1\")\n",
    "vol2.plot_pdf(\"2\")\n",
    "vol3.plot_pdf(\"3\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_rms(test_function, test_function_true_value, iterations=30, Ns=range(vol1.total_base_count, 6000, 200), \n",
    "         methods=[StratifiedMC(vol1, \"1\").get_interface_infer_multiple(), \n",
    "                  StratifiedMC(vol2, \"2\").get_interface_infer_multiple(), \n",
    "                  StratifiedMC(vol3, \"3\").get_interface_infer_multiple()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: It is most important to increase sampling in regions where f varies quickly.\n",
    "\n",
    "(Note: stratified sampling is substantially different from importance sampling, which is why for stratified sampling regions of large variance of f should have bigger weights, whereas for importance sampling the pdf should be as close to f as possible.\n",
    "In stratified sampling each region is considered individually and therefore when f is large but constant small samples will be acceptable. If $p(x)$ in importance sampling is small in these regions, these large values of f would be underrepresented and (as they yield large values of $f(x)/p(x)$) increase the variance of the MC estimate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "Consider a change of variables\n",
    "$$ \\int {dx \\; f(x)} = \\int {\\frac{f(x)}{p(x)} dx} = \\int {\\frac{f(x)}{p(x)} dP(x)} $$\n",
    "with $$ p(x) = \\frac{\\partial^d}{\\partial x_1 ... \\partial x_d} P(x) .$$\n",
    "\n",
    "Interpret $p(x)$ as a probability density with $p(x) \\geq 0$ and normalized: $\\int{dx\\;p(x)} = 1$. If the $x_n$ are sampled from the corresponding distribution $P(x)$, we have\n",
    "$$ E = \\frac{1}{N} \\sum_{n=1}^{N}{\\frac{f(x_n)}{p(x_n)}}.$$\n",
    "The variance of the MC estimate is (analogously to the first calculation)\n",
    "$$ \n",
    "\\int { dx_1...dx_N \\; p(x_1)...p(x_N)\\left( \\frac{1}{N} \\sum_{n=1}^N f(x_n)- I \\right)^2 } \n",
    "= \\frac{1}{N} \\int {dx \\; p(x) \\left( \\frac{f(x)}{p(x)} - I \\right)^2 } = \\frac{\\sigma^2(f/p)}{N} \\;,\n",
    "$$\n",
    "which may in turn be estimated by the sample variance, using E instead of I:\n",
    "$$ \n",
    "\\sigma^2\\left(\\frac{f}{p}\\right) \\approx S^2\\left(\\frac{f}{p}\\right) \n",
    "= \\frac{1}{N} \\sum_{n=1}^N {\\left(\\frac{f(x_n)}{p(x_n)}\\right)^2} - E^2 .\n",
    "$$\n",
    "\n",
    "Using $p$ which are (close to) zero where $f$ is not zero is dangerous as the variance diverges while the sample variance does not (appropriate regions are less likely to be sampled for small $p$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the default values (uniform distribution) monte carlo importance and plain monte carlo are exactly equivalent \n",
    "f = lambda x: np.sin(x*2*np.pi)**2\n",
    "plot_rms(f, 1/2, Ns=range(100, 5000, 100), iterations=10, methods=[PlainMC(), ImportanceMC(densities.Uniform(1))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Samples from a Given Distribution\n",
    "### Acceptance-Rejection method\n",
    "We want to sample according to a pdf $p(x)$. We know another (similar) pdf $h(x)$ s.t. $p(x) \\leq Ch(x)$ for some $C \\geq 0$.\n",
    "Then use the following algorithm:\n",
    "1. sample $x$ according to $h$\n",
    "2. generate random number $u \\in [0,1]$ and accept $x$ if $p(x) \\geq uCh(x)$, else reject and start over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D example\n",
    "a = 2\n",
    "p = lambda x:  np.exp(-a*x)  # want to sample according to this unnormalized pdf\n",
    "norm = a/(1-np.exp(-a))      # multiply p by this to normalize\n",
    "accept_reject = AcceptRejectSampler(p, 5)\n",
    "\n",
    "plt.hist(accept_reject.sample(10000).data, bins=10, normed=True)\n",
    "plt.plot(np.linspace(0,1,100), p(np.linspace(0,1,100))*norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis algorithm\n",
    "The metropolis algorithm is a simple Markov chain that, in it's limit, generates the desired distribution.\n",
    "It starts from a state $\\phi$ and transitions to a new state $\\phi'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Metropolis\n",
    "a =4\n",
    "p = lambda x: np.exp(-a*x[0])*np.sin(4*np.pi*x[1])**2  # unnormalized\n",
    "norm = a**2/(1-np.exp(-a))**2\n",
    "\n",
    "metropolis =  DefaultMetropolis(2, p)  # defaults to uniform proposal\n",
    "rand = metropolis.sample(100000, [.5, .5], log_every=10000).data.transpose()\n",
    "plt.hist2d(rand[0], rand[1], bins=50, normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply distributions to importance sampling\n",
    "dim = 1\n",
    "\n",
    "sin2_nd = lambda *x: sin_nd(*x)**2\n",
    "sin2_nd_trueval = lambda dim: .5**dim\n",
    "\n",
    "p_ideal = lambda *x: sin2_nd(*[xi+.0005 for xi in x]) / sin2_nd_trueval(dim)   # almost ideal\n",
    "accept_reject = AcceptRejectSampler(p_ideal, 1/sin2_nd_trueval(dim))  # max value of sin2_nd is 1\n",
    "sampling = lambda count: accept_reject.sample(count).data\n",
    "\n",
    "# create distribution\n",
    "dist = Distribution.make(ndim=dim, pdf_vect=p_ideal, rvs=sampling)\n",
    "\n",
    "plot_rms(sin2_nd, sin2_nd_trueval(dim), Ns=range(100, 1000, 100),  iterations=10, plot_fit=True,\n",
    "         methods=[PlainMC(), ImportanceMC(dist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a few distributions to use\n",
    "\n",
    "# compare importance sampling to the stratified boxing from before\n",
    "vol = GridVolumes(divisions=5, default_base_count=2, base_counts={(1,):32, (3,):32})\n",
    "\n",
    "p1 = lambda x: (bound(0,x,.2)*2 + bound(.2,x,.4)*32 + bound(.4,x,.6)*2 + bound(.6,x,.8)*32 + bound(.8,x,1)*2)/vol.total_base_count/.2\n",
    "p2 = lambda x: (bound(0,x,.2)*2 + bound(.2,x,.4)*22 + bound(.4,x,.6)*22 + bound(.6,x,.8)*22 + bound(.8,x,1)*2)/vol.total_base_count/.2\n",
    "p3 = lambda x: (bound(.2,x,.4)*((x-.2)/.2)**2 + bound(.6,x,.8)*((.8-x)/.2)**2 + bound(.4,x,.6))/(.2 + .1333333)\n",
    "sampling1 = AcceptRejectSampler(p1, 3)\n",
    "sampling2 = AcceptRejectSampler(p2, 2)\n",
    "sampling3 = AcceptRejectSampler(p3, 3.5)\n",
    "\n",
    "dist1 = Distribution.make(p1, 1, rvs=lambda count: sampling1.sample(count).data)\n",
    "dist2 = Distribution.make(p2, 1, rvs=lambda count: sampling2.sample(count).data)\n",
    "dist3 = Distribution.make(p3, 1, rvs=lambda count: sampling3.sample(count).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test_function and true_value from earlier\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Function\")\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x,test_function(x))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(x,p1(x), label=\"p1\")\n",
    "plt.plot(x,p2(x), label=\"p2\")\n",
    "plt.plot(x,p3(x), label=\"p3\")\n",
    "vol.plot_pdf(\"stratified\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_rms(test_function, test_function_true_value, iterations=10, Ns=range(vol.total_base_count, vol.total_base_count*100, vol.total_base_count*10), \n",
    "         methods=[StratifiedMC(volumes=vol, name=\"stratified\").get_interface_infer_multiple(),\n",
    "                  ImportanceMC(dist1, name=\"p1\"),\n",
    "                  ImportanceMC(dist2, name=\"p2\"),\n",
    "                  ImportanceMC(dist3, name=\"p3\"),\n",
    "                  PlainMC()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Methods\n",
    "### VEGAS Monte Carlo\n",
    "\n",
    "Vegas Monte Carlo uses the same GridVolumes as the stratified sampling examples from before.\n",
    "During the integration, the widths of the grid in each dimension are updated such that each division would contribute equally much. This leads to a denser grid around peaks of the integrand. The total pdf will approximate the integral as much as possible within the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect how optimization impacts errors\n",
    "trueval = .25\n",
    "f = lambda x,y: np.sin(2 * np.pi * x)**2 * np.sin(2 * 2 * np.pi * y)**2\n",
    "\n",
    "mc_vegas = VegasMC(divisions=30, c=20, ndim=2, var_weighted=False)\n",
    "\n",
    "Ns = np.logspace(2, 3, 10, dtype=np.int)\n",
    "# plot errors\n",
    "plot_rms(f, trueval, Ns=Ns, iterations=70, methods=[mc_vegas.get_interface_infer_multiple(10)], plot_fit=True)\n",
    "plt.show()\n",
    "\n",
    "# plot distribution\n",
    "plot_distribution(f, trueval, 1000, 1000, mc_vegas.get_interface_infer_multiple(100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100) \n",
    "mgrid = np.meshgrid(x, y)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(f(*mgrid), extent=(-1, 1, -1, 1))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(mc_vegas.volumes(*mgrid), extent=(-1, 1, -1, 1))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegas3 = VegasMC(divisions=10, c=3, name=\"VEGAS c=3\").get_interface_infer_multiple(100)\n",
    "vegas5 = VegasMC(divisions=10, c=10, name=\"VEGAS c=10\").get_interface_infer_multiple(100)\n",
    "plot_rms(test_function, test_function_true_value, iterations=20, Ns=range(400, 4000, 100),\n",
    "         methods=[vegas3, vegas5, PlainMC()], plot_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegas = VegasMC(divisions=300, c=400)\n",
    "sample = vegas(test_function, sub_eval_count=1000, iterations=50, chi=True)\n",
    "est, err, chi2 = sample.integral, sample.integral_err, sample.chi2\n",
    "print(est, ' +- ', err)\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x, test_function(x) / est)\n",
    "plt.plot(x, vegas.volumes.pdf(x))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Channel Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-channel Monte Carlo is an adaptive Monte Carlo integration technique based on importance sampling. \n",
    "Instead of one probability distribution, the multi-channel technique comprises a set of distributions $p_k$ (called channels) \n",
    "with respective weights $\\alpha_k$. For each step, a channel is chosen randomly with probability $\\alpha_k$, and $x_i$ is \n",
    "(pseudo-) randomly selected according to the chosen channel.\n",
    "\n",
    "The integration process is split into iterations with $N_j$ function evaluations in each, such that after one iteration \n",
    "the channel weights can be updated to minimize the variance of $f/p$, where $p(x) = \\sum_{k=1}^m \\alpha_i p_k(x)$ is the \n",
    "overall or total probability distribution.\n",
    "\n",
    "For each iteration, define\n",
    "$$ W_{j, k}(\\alpha) \n",
    "= \\int_V \\text{d} x \\; p_j(x) \\left(\\frac{f(x)}{p(x)}\\right)^2\n",
    "\\approx \\frac{V}{N_j} \\sum_{i=1}^{N_j} { \\left( \\frac{f(x_i)}{p(x_i} \\right) } , \\\\\n",
    "W_j(\\alpha)\n",
    " = \\int_V \\text{d} x \\; p(x) \\left(\\frac{f(x)}{p(x)}\\right)^2\n",
    "\\approx \\sum_{k=1}^m \\alpha_k W_{j,k} $$\n",
    "\n",
    "where the right sides are the Monte Carlo estimates. $W_{j,k}$ can be understood as the contribution of channel $j$ \n",
    "to the total variance of the integrand. To reduce the variance, channels that contribute disproportionately much should \n",
    "have a larger weight. The channels are update according to\n",
    "\n",
    "$$\\alpha_k^{new} = \\frac{\\alpha_k (W_{j,k}(\\alpha))^\\beta}{\\sum_k {\\alpha_k (W_{j,k}(\\alpha))^\\beta}}, $$\n",
    "\n",
    "with $\\beta$ ranging from $1/2$ to $1/4$.\n",
    "\n",
    "Analogous to importance sampling the integration estimate for one iteration is\n",
    "\n",
    "$$ E_j = \\frac{V}{N_j} \\sum_{i=1}^{N_j} { \\frac{f(x_i)}{p(x_i)} }, \\\\\n",
    "\\sigma_{E_j}^2 = \\frac{W_j(\\alpha) - I^2}{N} . $$\n",
    "\n",
    "Since the estimate is independent of the channel weights, the estimates from all iterations can be combined into a total estimate:\n",
    "$$ E = \\frac{1}{N} \\sum_j {N_j E_j} ,  \\\\\n",
    "\\sigma_E^2 = \\frac{\\sum_j \\frac{Nj}{N}W_j - I^2}{N} , $$\n",
    "where $N = \\sum_j {N_j}$.\n",
    "\n",
    "Depending on the problem it might be useful to have an initial phase of weight optimization that does not contribute to the total estimate, or an additional final phase in which the weights are not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test channel sampling\n",
    "channels = MultiChannel([Distribution.make(ndim=3, pdf_vect=lambda *x: np.ones_like(x[0]), \n",
    "                                           rvs=lambda N: np.random.rand(N*3).reshape(N,3)),\n",
    "                         Distribution.make(ndim=3, pdf_vect=lambda x, y, z: 2*(z<=.5)*(x<=.5)*(y<=.5), \n",
    "                                           rvs=lambda N: np.random.rand(N*3).reshape(N,3)/2)])\n",
    "sample = channels.sample(1000)\n",
    "# example of computing the mean, answer should be .5 in upper row, .25 in lower\n",
    "np.add.reduceat(sample.data, sample.channel_bounds)  / sample.count_per_channel[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use on test_function with the three probability distributions from before\n",
    "channels = MultiChannel([dist1, dist2, dist3])\n",
    "\n",
    "mcmci = MultiChannelMC(channels).get_interface_ratios(sub_eval_count=100, r1=0, r2=.5, r3=.5)\n",
    "\n",
    "Ns = np.logspace(2, 5, 15, dtype=np.int)\n",
    "plot_rms(test_function, test_function_true_value, Ns=Ns, iterations=20, \n",
    "               methods=[mcmci, PlainMC()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(test_function, test_function_true_value, 1e3, 1000, mcmci, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = .1\n",
    "sampling3_shifted = AcceptRejectSampler(lambda x: p3(x-shift), 3.5).sample\n",
    "dist3_shifted = Distribution.make(lambda x: p3(x-shift), 1, rvs=lambda count: sampling3_shifted(count).data)\n",
    "channels = MultiChannel([dist1, dist2, dist3_shifted])\n",
    "mcmci = MultiChannelMC(channels).get_interface_ratios(sub_eval_count=100, r1=.2, r2=.4, r3=.4)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "x = np.linspace(0,1,1000)\n",
    "plt.plot(x, p1(x), label=\"p1\")\n",
    "plt.plot(x, p2(x), label=\"p2\")\n",
    "plt.plot(x, p3(x), label=\"p3\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "sample = mcmci(test_function, 10000)\n",
    "est, err = sample.integral, sample.integral_err\n",
    "\n",
    "print(est-test_function_true_value, err)\n",
    "channels.plot_pdf()\n",
    "plt.plot(x, test_function(x), label=\"function\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "While in the above examples the`AcceptReject` class was used to sample according to a specific distribution for\n",
    "Monte Carlo integration, in real applications distributions with a known inverse would be used, together with the inverse transfrom\n",
    "method.\n",
    "\n",
    "The following method (and Markov chain/Metropolis algorithms genrally) are used to construct samples according to more \n",
    "complicated distributions, when the sample creation is the main goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Markov Chains\n",
    "\n",
    "In the following are some simple examples of Markov chains generated using Metropolis algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: uniform proposal and uniform pdf metropolis\n",
    "metrop_uniform = DefaultMetropolis(1, lambda x: 1)\n",
    "r = metrop_uniform.sample(500, 0)\n",
    "\n",
    "plt.title(\"time series plot\")\n",
    "plt.plot(r.data)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot histogram\n",
    "plt.subplot(121)\n",
    "plt.title(\"histogram\")\n",
    "_ = plt.hist(r.data, bins=20, normed=True)\n",
    "\n",
    "# plot autocorrelation\n",
    "plt.subplot(122)\n",
    "plt.title(\"autocorrelation\")\n",
    "acov = util.auto_corr(r.data)\n",
    "plt.bar(np.arange(len(acov)), acov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: local proposal and uniform pdf metropolis\n",
    "delta = .1\n",
    "proposal_generator = lambda s: min(max(0, s-delta/2), 1-delta) + np.random.rand()*delta\n",
    "proposal_pdf = lambda x, y: 1/delta\n",
    "pdf = lambda x: np.exp(-(x-.5)**2/.01)\n",
    "norm = 0.177245 # pdf / norm is normalized\n",
    "metrop_gauss1 = DefaultMetropolis(1, pdf, Proposal.make(proposal_generator, 1, proposal_pdf))\n",
    "\n",
    "r = metrop_gauss1.sample(10000, .5).data\n",
    "plt.title(\"time series plot\")\n",
    "plt.plot(r)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot histogram\n",
    "plt.subplot(121)\n",
    "plt.title(\"histogram\")\n",
    "_ = plt.hist(r, bins=20, normed=True)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, pdf(x)/norm)\n",
    "\n",
    "# plot autocorrelation\n",
    "plt.subplot(122)\n",
    "plt.title(\"autocorrelation\")\n",
    "acov = util.auto_corr(r)\n",
    "plt.bar(np.arange(len(acov)), acov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: local proposal and uniform pdf metropolis, proposal area 1/5th as large\n",
    "delta = .02\n",
    "proposal = lambda s: min(max(0, s-delta/2), 1-delta) + np.random.rand()*delta\n",
    "proposal_pdf = lambda x, y: 1/delta\n",
    "metrop_gauss02 = DefaultMetropolis(1, pdf, Proposal.make(proposal, 1, proposal_pdf))\n",
    "\n",
    "r = metrop_gauss02.sample(10000, .5).data[::1]\n",
    "plt.title(\"time series plot\")\n",
    "plt.plot(r)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot histogram\n",
    "plt.subplot(121)\n",
    "plt.title(\"histogram\")\n",
    "_ = plt.hist(r, bins=20, normed=True)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, pdf(x)/norm)\n",
    "\n",
    "# plot autocorrelation\n",
    "plt.subplot(122)\n",
    "plt.title(\"autocorrelation\")\n",
    "acov = util.auto_corr(r)\n",
    "plt.bar(np.arange(len(acov)), acov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: local proposal and uniform pdf metropolis, proposal area 1/5th as large, now 10 times as many samples, subsample\n",
    "delta = .02\n",
    "proposal_generator = lambda s: min(max(0, s-delta/2), 1-delta) + np.random.rand()*delta\n",
    "proposal_pdf = lambda x, y: 1/delta\n",
    "metrop_gauss02 = DefaultMetropolis(1, pdf, Proposal.make(proposal_generator, 1, proposal_pdf))\n",
    "\n",
    "r = metrop_gauss02.sample(100000, .5, log_every=10000).data[::10]\n",
    "plt.title(\"time series plot\")\n",
    "plt.plot(r)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot histogram\n",
    "plt.subplot(121)\n",
    "plt.title(\"histogram\")\n",
    "_ = plt.hist(r, bins=20, normed=True)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, pdf(x)/norm)\n",
    "\n",
    "# plot autocorrelation\n",
    "plt.subplot(122)\n",
    "plt.title(\"autocorrelation\")\n",
    "acov = util.auto_corr(r)\n",
    "plt.bar(np.arange(len(acov)), acov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo\n",
    "Especially in statistics one might want to compute an integral such as\n",
    "$$ \\int f(x) p(x) \\text{d}x, $$\n",
    "where $p(x)$ is a probability distribution.\n",
    "This can simply be done by using a Markov Chain to generate samples according to $p$. The main goal would the be to find a\n",
    "Markov Chain algorithm (the appropriate update mechanism) with $p$ as equilibrium distribution.\n",
    "The Markov Chain method is not further discussed here, but below is a simple implementation followed by an example.\n",
    "\n",
    "See for example [here](http://mcmchandbook.net/) for an introduction to Markov Chain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC(object):\n",
    "    def __init__(self, batch_length, generator, initial):\n",
    "        self.batch_length = batch_length\n",
    "        self.generator = generator\n",
    "        self.state = initial\n",
    "        \n",
    "    def sample_function(self, fn, batch_count):\n",
    "        batch_means = np.empty(batch_count)\n",
    "        for i in range(batch_count):\n",
    "            xs = self.generator.sample(self.batch_length, self.state).data\n",
    "            self.state = xs[-1]\n",
    "            fn_values = fn(*xs.transpose())\n",
    "            # CLT: batch_mean is distributed according to normal(actual mean, batch_vars/batch_length) \n",
    "            # (if MC is reversible, stationary)\n",
    "            batch_means[i] = np.mean(fn_values)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"batch means\")\n",
    "        plt.plot(batch_means, \"o\")\n",
    "        plt.xlabel(\"N\")\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"ACF of batch means\")\n",
    "        acov = util.auto_corr(batch_means)\n",
    "        plt.bar(np.arange(len(acov)), acov)\n",
    "        plt.xlabel(\"N\")\n",
    "        plt.show()\n",
    "        \n",
    "        return batch_means\n",
    "    \n",
    "    def __call__(self, fn, N):\n",
    "        assert N > self.batch_length, \"N must be at least the size of a batch\"\n",
    "        batch_count = N // self.batch_length\n",
    "        N = batch_count * self.batch_length\n",
    "        \n",
    "        batch_means = self.sample_function(fn, batch_count)\n",
    "        mean = np.mean(batch_means)  # same as fn(chain) / N, since batch_count is constant throughout\n",
    "        var_omc = np.mean((batch_means - mean)**2)  # estimates variance of fn / batch_length (i.e. variance of batch mean of fn)\n",
    "        var_dependence = var_omc + 2 * np.mean(util.auto_cov(batch_means)[1:]) # this is probably not what we want\n",
    "        \n",
    "        # variance of the mean estimate is lag_cov(0) + 2 sum_k{ lag_cov(k) }\n",
    "        # where lag_cov(k) is estimated by 1/m sum_i (batch_means_i - mean)(batch_means_{i+k} - mean)\n",
    "        # ALTERNATIVELY: use smaller (thus better) convex minorant\n",
    "        # this works because the sequence of batches is also a Markov chain and thus the CLT holds\n",
    "        # i.e. here we estimate the mean batch value of fn, which (if the chain is stationary) is just the mean of fn\n",
    "        return mean, np.sqrt(var_omc / batch_count), np.sqrt(var_dependence / batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = .05\n",
    "proposal_generator = lambda s: min(max(0, s-delta/2), 1-delta) + np.random.rand()*delta\n",
    "proposal_pdf = lambda x, y: 1/delta\n",
    "pdf = lambda x: np.exp(-(x-.5)**2/(2*0.1**2))\n",
    "metrop_gauss05 = DefaultMetropolis(1, pdf, Proposal.make(proposal_generator, 1))\n",
    "\n",
    "mcmc = MCMC(1000, metrop_gauss05, 0.5)\n",
    "mcmc(lambda x: (x-.5)**2, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mulit-channel Markov chain Monte Carlo ($\\mathrm{MC^3}$) method combines \n",
    "multi-channel Monte Carlo integration with the Metropolis-Hasting algorithm to efficiently generate a sample according \n",
    "to the unnormalized distribution of a function $f$, which is expensive to evaluate. \n",
    "\n",
    "\n",
    "In the first phase, the function $f$ is integrated over the sample space using multi-channel Monte Carlo. \n",
    "The channels used for the integration are passed to the algorithm (and constructed using knowledge about the function such \n",
    "as sizes and locations of peaks, which physically correspond to resonances in the matrix element). As a result of the multi-channel\n",
    "integration the channel weights are optimized, leading to a combined probability distribution $p_{IS}$ which approximates $f$. \n",
    "The total integral of $f$ may be of interest to users of the algorithm, but is unimportant for the sample generation.\n",
    "\n",
    "The sampling, which follows the integration phase, generates a Markov Chain that mixes two Metropolis-Hasting update mechanisms. \n",
    "The first update mechanism is chosen with probability $\\beta$, and is a Metropolis-Hasting update with candidates proposed \n",
    "according to the distribution $p_{IS}$ from the multi-channel integration (thus independently of the previous state in the chain). \n",
    "The second, chosen with probability $1-\\beta$, is a Metropolis update that uses some local (and symmetric) proposal distribution \n",
    "$p_{loc}(x|y)$. The combined update mechanism is reversible and has the desired equilibrium distribution $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = lambda x: np.sin(2*np.pi*x)**2 * np.sin(10*2*np.pi*x)**2\n",
    "channels = MultiChannel([dist2, dist3])\n",
    "mc3_sampler = mc3.MC3Uniform(fn, channels, delta=.01, beta=1)\n",
    "# beta = 1: only importance sampling\n",
    "res = mc3_sampler(([], [500]*40, []), 30000).data\n",
    "\n",
    "plt.title(\"Generated events\")\n",
    "plt.hist(res, normed=True, bins=100)\n",
    "plt.xlim(0, 1)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, fn(x)/mc3_sampler.integration_sample.integral)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Channels\")\n",
    "mc3_sampler.channels.plot_pdf()\n",
    "plt.plot(x, fn(x)/mc3_sampler.integration_sample.integral)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
