{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepmc import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nsamples = 1000\n",
    "\n",
    "target = densities.Camel(ndim)\n",
    "# add .count attributes to pdf and pot_gradient methods\n",
    "util.count_calls(target, 'pdf', 'pot_gradient')\n",
    "start = np.full(ndim, 1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Updates\n",
    "The package defines the abstract `MarkovUpdate` which implements a general `sample` function\n",
    "based on the `next_state(current)` function, that each concrete update must override.\n",
    "The sample returned is an instance of `MarkovSample` and contains the generated sample as `data`\n",
    "and provides information such as the mean, variance, and acceptance rate for Metropolis updates.\n",
    "\n",
    "## Metropolis Updates\n",
    "Metropolis updates are generally subclasses of `MetropolisUpdate`, which is abstract.\n",
    "The \"default\" Metropolis (Hasting) update is implemented in `DefaultMetropolis`.\n",
    "The metheods, on instantiation, take the sample dimension and a callable target probability\n",
    "as arguments. The pdf can be a `Density` object, but as opposed to Hamiltonian updates, they\n",
    "can also be simple functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Metropolis Hasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "# by default the range for Uniform ins [0, 1]^ndim\n",
    "proposal = densities.Uniform(ndim)\n",
    "sampler = DefaultMetropolis(ndim, target, proposal)\n",
    "\n",
    "# sample\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "# print info\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calls: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample  # show sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadapt = 1000\n",
    "nburnin = 1000\n",
    "\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "# hast to be a proposal with cov attribute\n",
    "# this is what adaptive metropolis modifies\n",
    "proposal = densities.Gaussian(ndim, mu=0.5, cov=0.005)\n",
    "metropolis_sampler = AdaptiveMetropolisUpdate(\n",
    "    ndim, target.pdf, proposal, t_initial=100,\n",
    "    adapt_schedule=lambda t: t <= nadapt)\n",
    "\n",
    "# burn in\n",
    "metropolis_sampler.sample(nburnin, [0.5, 0.5], log_every=0)\n",
    "metropolis_sampler.is_adaptive = False  # turn adaptation off\n",
    "\n",
    "t_start = timer()\n",
    "sample = metropolis_sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "sample.target = target\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calls: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Updates\n",
    "Hamiltonian updates take densities as argument, and therefore don't require a dedicated ndim argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.0041\n",
    "steps = 40\n",
    "\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "momentum_dist = densities.Gaussian(ndim, scale=1)\n",
    "sampler = hamiltonian.HamiltonianUpdate(target, momentum_dist, steps, step_size)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nsamples = 1000\n",
    "\n",
    "target = densities.Gaussian(ndim, [1/3, 1/3], cov=0.1**2/2)\n",
    "# add .count attributes to pdf and pot_gradient methods\n",
    "util.count_calls(target, 'pdf', 'pot_gradient')\n",
    "start = np.full(ndim, 1/3)\n",
    "\n",
    "step_size = .0074\n",
    "steps = 40\n",
    "    \n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "momentum_dist = densities.Gaussian(ndim, cov=1)\n",
    "sampler = hamiltonian.HamiltonianUpdate(target, momentum_dist, steps, step_size)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, [.4, .4], log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "print(util.bin_wise_chi2(sample, 20))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nsamples = 1000\n",
    "\n",
    "target = densities.Camel(2)\n",
    "# add .count attributes to pdf and pot_gradient methods\n",
    "util.count_calls(target, 'pdf', 'pot_gradient')\n",
    "start = np.full(ndim, 1/3)\n",
    "\n",
    "step_size = .0074\n",
    "steps = 40\n",
    "    \n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "beta = 0.7\n",
    "momentum_dist = densities.Gaussian(ndim, cov=1)\n",
    "sampler_local = hamiltonian.HamiltonianUpdate(target, momentum_dist, steps, step_size)\n",
    "sampler_is = DefaultMetropolis(2, target)\n",
    "sampler = MixingMarkovUpdate(2, [sampler_is, sampler_local], [beta, 1-beta])\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, [.4, .4], log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "print(util.bin_wise_chi2(sample, 20))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nsamples = 1000\n",
    "\n",
    "target = densities.Gaussian(ndim, [1/3, 1/3], cov=0.1**2/2)\n",
    "# add .count attributes to pdf and pot_gradient methods\n",
    "util.count_calls(target, 'pdf', 'pot_gradient')\n",
    "start = np.full(ndim, 1/3.01)\n",
    "\n",
    "nadapt = 500\n",
    "\n",
    "def adapt_schedule(t):\n",
    "    return t <= nadapt\n",
    "\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "momentum_dist = densities.Gaussian(ndim, scale=50)\n",
    "sampler = hamiltonian.NUTSUpdate(target, momentum_dist, adapt_schedule)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.step_size_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Averaging HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadapt = 1000\n",
    "\n",
    "def adapt_schedule(t):\n",
    "    return t <= nadapt\n",
    "\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "momentum_dist = densities.Gaussian(ndim, scale=.05)\n",
    "sampler = hamiltonian.DualAveragingHMC(target, momentum_dist, 1, adapt_schedule)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-U-turn sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadapt = -1\n",
    "\n",
    "def adapt_schedule(t):\n",
    "    return t <= nadapt\n",
    "\n",
    "target = densities.Gaussian(ndim, [1/3, 1/3], cov=0.1**2/2)\n",
    "util.count_calls(target, 'pdf', 'pot_gradient')\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pot_gradient.count = 0\n",
    "\n",
    "momentum_dist = densities.Gaussian(ndim, scale=50)\n",
    "sampler = hamiltonian.NUTSUpdate(target, momentum_dist, adapt_schedule)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spherical HMC\n",
    "Sampling is by implementation restrained to $[0, 1]^{ndim}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "sampler = hamiltonian.StaticSphericalHMC(target, 0.01, 1, 1, 15)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spherical NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadapt = 100\n",
    "\n",
    "def adapt_schedule(t):\n",
    "    return t <= nadapt\n",
    "\n",
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "sampler = hamiltonian.SphericalNUTS(target, adapt_schedule)\n",
    "\n",
    "t_start = timer()\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "print('time: ', t_end - t_start)\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling - Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopt = 1000  # integration steps\n",
    "\n",
    "# imperfect channels\n",
    "channels = MultiChannel([\n",
    "    densities.Gaussian(ndim, mu=1/5, cov=0.005),\n",
    "    densities.Gaussian(ndim, mu=4/5, cov=0.005)])\n",
    "\n",
    "# multi channel integrator\n",
    "importance = MultiChannelMC(channels)\n",
    "\n",
    "t_start = timer()\n",
    "integration_sample = importance(target, [], [nopt], []) # integrate\n",
    "\n",
    "sampler = DefaultMetropolis(ndim, target.pdf, channels)\n",
    "sample = sampler.sample(nsamples, start, log_every=nsamples/4)\n",
    "t_end = timer()\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC 3\n",
    "### Uniform local sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [densities.Gaussian(ndim, mu=np.random.rand(2), scale=0.2) for i in range(10)]\n",
    "sampler = mc3.MC3Uniform(target, MultiChannel(dist), 0.1, beta=.8)\n",
    "sample = sampler(([], [1000], []), nsamples)\n",
    "sample.target = target\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC local sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset counts\n",
    "target.pdf.count = 0\n",
    "target.pdf.pot_gradient = 0\n",
    "\n",
    "step_size = .0041\n",
    "steps = 40\n",
    "\n",
    "dist = [densities.Gaussian(ndim, mu=np.random.rand(ndim), scale=0.2) for i in range(10)]\n",
    "sampler = mc3.MC3Hamilton(target, MultiChannel(dist), 1., steps, step_size, beta=.2)\n",
    "sample = sampler(([], [1000], []), 1000, log_every=1000/4)\n",
    "sample.target = target\n",
    "\n",
    "print('pdf calss: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.bin_wise_chi2(sample, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometric basis surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "step_size = 0.1\n",
    "mass = 10\n",
    "beta = 0\n",
    "nodes = 1000\n",
    "\n",
    "# reset counts\n",
    "target_s = densities.Camel(ndim)\n",
    "\n",
    "# importance\n",
    "channels = MultiChannel([densities.Gaussian(ndim, mu=np.random.rand(2), scale=0.2) for i in range(10)])\n",
    "mc_importance = MultiChannelMC(channels)\n",
    "integration_sample = mc_importance(target_s, [], [10000], [])\n",
    "\n",
    "# surrogate\n",
    "basis = surrogate.extreme_learning.TrigBasis(ndim)\n",
    "log_vals = -np.ma.log(integration_sample.function_values)\n",
    "xvals = integration_sample.data[~log_vals.mask]\n",
    "log_vals = log_vals[~log_vals.mask]\n",
    "\n",
    "params = basis.extreme_learning_train(xvals, log_vals, nodes)\n",
    "def surr(xs):\n",
    "    return basis.eval_gradient(*params, xs)\n",
    "target_s.pot_gradient = surr\n",
    "\n",
    "\n",
    "# sampling\n",
    "p_dist = densities.Gaussian(channels.ndim, scale=mass)\n",
    "sample_hmc = hamiltonian.HamiltonianUpdate(target_s, p_dist, steps, step_size)\n",
    "\n",
    "sample_is = DefaultMetropolis(ndim, target_s.pdf, channels)\n",
    "\n",
    "updates = [sample_is, sample_hmc]\n",
    "mixing_sampler = MixingMarkovUpdate(ndim, updates, [beta, 1-beta])\n",
    "\n",
    "\n",
    "sample = mixing_sampler.sample(nsamples, [0.3, 0.4], log_every=nsamples/4)\n",
    "sample.target = target\n",
    "\n",
    "print('pdf calls: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100) \n",
    "mgrid = np.meshgrid(x, y)\n",
    "\n",
    "# fit\n",
    "out = basis.eval_gradient_split(*params, *mgrid)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(221)\n",
    "plt.title(\"surrogate dx\")\n",
    "plt.imshow(out[:,:,0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"surrogate dy\")\n",
    "plt.imshow(out[:,:,1])\n",
    "plt.colorbar()\n",
    "\n",
    "grad = target.pot_gradient(np.stack((mgrid[0].flatten(), mgrid[1].flatten()), axis=0).transpose()).reshape((*mgrid[0].shape, 2))\n",
    "plt.subplot(223)\n",
    "plt.title(\"real dx\")\n",
    "plt.imshow(grad[:,:,0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"real dy\")\n",
    "plt.imshow(grad[:,:,1])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian radial basis surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 30\n",
    "step_size = 0.1\n",
    "mass = 10\n",
    "beta = 0.1\n",
    "nodes = 1000\n",
    "\n",
    "# reset counts\n",
    "target_s = densities.Camel(ndim)\n",
    "\n",
    "# importance\n",
    "channels = MultiChannel([densities.Gaussian(ndim, mu=np.random.rand(2), scale=0.2) for i in range(10)])\n",
    "mc_importance = MultiChannelMC(channels)\n",
    "integration_sample = mc_importance(target_s, [], [10000], [])\n",
    "\n",
    "# surrogate\n",
    "basis = surrogate.extreme_learning.GaussianBasis(ndim)\n",
    "log_vals = -np.ma.log(integration_sample.function_values)\n",
    "xvals = integration_sample.data[~log_vals.mask]\n",
    "log_vals = log_vals[~log_vals.mask]\n",
    "\n",
    "params = basis.extreme_learning_train(xvals, log_vals, nodes)\n",
    "def surr(xs):\n",
    "    return basis.eval_gradient(*params, xs)\n",
    "target_s.pot_gradient = surr\n",
    "\n",
    "\n",
    "# sampling\n",
    "p_dist = densities.Gaussian(channels.ndim, scale=mass)\n",
    "sample_hmc = hamiltonian.HamiltonianUpdate(target_s, p_dist, steps, step_size)\n",
    "\n",
    "sample_is = DefaultMetropolis(ndim, target_s, channels)\n",
    "\n",
    "updates = [sample_is, sample_hmc]\n",
    "mixing_sampler = MixingMarkovUpdate(ndim, updates, [beta, 1-beta])\n",
    "\n",
    "\n",
    "sample = mixing_sampler.sample(nsamples, [0.3, 0.4], log_every=nsamples/4)\n",
    "sample.target = target\n",
    "\n",
    "print('pdf calls: ', target.pdf.count)\n",
    "print('pot_gradient calls: ', target.pot_gradient.count)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100) \n",
    "mgrid = np.meshgrid(x, y)\n",
    "\n",
    "# fit\n",
    "out = basis.eval_gradient_split(*params, *mgrid)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(221)\n",
    "plt.title(\"surrogate dx\")\n",
    "plt.imshow(out[:,:,0], vmin=-120, vmax=120)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"surrogate dy\")\n",
    "plt.imshow(out[:,:,1], vmin=-120, vmax=120)\n",
    "plt.colorbar()\n",
    "\n",
    "grad = target.pot_gradient(np.stack((mgrid[0].flatten(), mgrid[1].flatten()), axis=0).transpose()).reshape((*mgrid[0].shape, 2))\n",
    "plt.subplot(223)\n",
    "plt.title(\"real dx\")\n",
    "plt.imshow(grad[:,:,0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"real dy\")\n",
    "plt.imshow(grad[:,:,1])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
