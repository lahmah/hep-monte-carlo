{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepmc.core.densities.qcd import ee_qq_ng, export_hepmc\n",
    "from hepmc.core.densities.sarge import Sarge\n",
    "from hepmc.core.phase_space.rambo import RamboOnDiet\n",
    "from scipy.special import logit, expit\n",
    "from hepmc.core.phase_space.mapping import MappedDensity\n",
    "from hepmc.core.markov.metropolis import DefaultMetropolis\n",
    "from hepmc.core.markov.base import MixingMarkovUpdate\n",
    "from hepmc.core.markov.metropolis_adaptive import AdaptiveMetropolisUpdate\n",
    "from hepmc.core.proposals import Gaussian\n",
    "from hepmc import surrogate\n",
    "from hepmc.core import densities\n",
    "from hepmc.core.hamiltonian.hmc import HamiltonianUpdate\n",
    "from hepmc.core.hamiltonian.wall_hmc import WallHMC\n",
    "from hepmc.core.sampling import Sample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarge = densities.Sarge(2, 3, 100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeqqg = ee_qq_ng(1, 100., 5., .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo_mapping = RamboOnDiet(100., 3)\n",
    "mapped = MappedDensity(eeqqg, rambo_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sarge_sample = ImportanceMC(sarge)(eeqqg.pdf, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(sarge_sample, sarge_sample.mean, sarge_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities.export_hepmc(100., sarge_sample, \"../samples/qcd/2-3/sarge_weighted.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = sarge_sample.weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# acceptance-rejection sampler\n",
    "sarge_sample_unweighted = np.empty((10000, 12))\n",
    "for i in range(10000):\n",
    "    aprob = 0\n",
    "    while np.random.random() > aprob:\n",
    "        proposal = sarge.proposal()\n",
    "        f = eeqqg.pdf(proposal)\n",
    "        g = sarge.proposal_pdf(None, proposal)\n",
    "        aprob = f/(bound*g)\n",
    "        if aprob > 1:\n",
    "            print(\"bound too low\")\n",
    "    sarge_sample_unweighted[i] = proposal\n",
    "sarge_sample_unweighted = Sample(data=sarge_sample_unweighted, weights=np.full(10000, 1./10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities.export_hepmc(100., sarge_sample_unweighted, \"../samples/qcd/2-3/sarge_unweighted.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo_sample = PlainMC(5)(mapped, 10000)\n",
    "rambo_sample.integral, rambo_sample.integral_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = mapped.pdf(rambo_sample.data)\n",
    "max_y = ys.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo = AcceptRejectSampler(mapped, max_y, 5)\n",
    "%time rambo_sample = rambo.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rambo_sample.data = rambo_mapping.map(rambo_sample.data)\n",
    "rambo_sample.weights = np.full(rambo_sample.size, 1./rambo_sample.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities.export_hepmc(100., rambo_sample, \"../samples/qcd/2-3/rambo.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis with Rambo mapping\n",
    "met = DefaultMetropolis(mapped, Gaussian(5, .01))\n",
    "#start = met.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "met_sample = met.sample(10000, np.random.rand(5), burnin=5000, lag=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(met_sample, met_sample.mean, met_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_sample.data = rambo_mapping.map(met_sample.data)\n",
    "met_sample.weights = np.full(met_sample.size, 1./met_sample.size)\n",
    "densities.export_hepmc(100., met_sample, \"../samples/qcd/2-3/metropolis.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive Metropolis\n",
    "def adapt_schedule(t):\n",
    "    if t < 50000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "met = AdaptiveMetropolisUpdate(5, mapped, proposals.Gaussian(5, .01), t_initial=10000, adapt_schedule=adapt_schedule)\n",
    "%time met_sample = met.sample(1060000, np.random.rand(5))\n",
    "met_sample.data = met_sample.data[60000:] # discard warmup samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thin sample\n",
    "met_sample.data = met_sample.data[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_sample.data = rambo_mapping.map(met_sample.data)\n",
    "met_sample.weights = np.full(met_sample.size, 1./met_sample.size)\n",
    "densities.export_hepmc(100., met_sample, \"../samples/qcd/2-3/metropolis.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(met_sample, met_sample.mean, met_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual averaging Metropolis\n",
    "def adapt_schedule(t):\n",
    "    if t < 5000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "met = StochasticOptimizeUpdate(mapped, proposals.Gaussian(5, .01), adapt_schedule=adapt_schedule)\n",
    "%time met_sample = met.sample(16000, np.random.rand(5))\n",
    "met_sample.data = met_sample.data[6000:] # discard warmup samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(met_sample, met_sample.mean, met_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sarge + local in MC3\n",
    "importance = DefaultMetropolis(3 * 4, eeqqg, proposal=sarge)  # using sarge, full momentum sample space\n",
    "local = DefaultMetropolis(5, mapped, proposals.Gaussian(5, .01))  # using rambo, rambo sample space\n",
    "update = MixingMarkovUpdate(5, [importance, local], target=mapped,\n",
    "                            out_maps={0: rambo_mapping.map_inverse},\n",
    "                            in_maps={0: rambo_mapping.map}) # rambo sample space, important to set target explicitly\n",
    "start = update.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "%time mc3_sample = update.sample(10000, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated ESS:', util.effective_sample_size(mc3_sample, mc3_sample.mean, mc3_sample.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc3_sample.data = rambo_mapping.map(mc3_sample.data)\n",
    "mc3_sample.weights = np.full(mc3_sample.size, 1./mc3_sample.size)\n",
    "densities.export_hepmc(100., mc3_sample, \"../samples/qcd/2-3/mc3.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
