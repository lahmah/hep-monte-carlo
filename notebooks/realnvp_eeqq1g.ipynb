{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "#tfd = tf.contrib.distributions\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "layers = tf.contrib.layers\n",
    "tf.set_random_seed(0)\n",
    "from hepmc.core.densities.qcd import ee_qq_ng, export_hepmc, import_hepmc\n",
    "from hepmc.core.densities.nice import Nice\n",
    "from hepmc.core.phase_space.rambo import RamboOnDiet\n",
    "from hepmc.core.phase_space.mapping import MappedDensity\n",
    "from hepmc.core.sampling import Sample\n",
    "from hepmc.core.integration.importance import ImportanceMC\n",
    "from hepmc.core.sampling import Sample, AcceptRejectSampler\n",
    "#from hepmc.core.markov.metropolis import DefaultMetropolis\n",
    "#from hepmc.core.proposals import Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherpa_weighted_sample = import_hepmc('../samples/qcd/2-3/sherpa_weighted.hepmc')\n",
    "sherpa_weighted_sample = Sample(data=rambo_mapping.map_inverse(sherpa_weighted_sample.data), weights=sherpa_weighted_sample.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(sherpa_weighted_sample.data)\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sherpa_weighted_sample.weights, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherpa_weighted_sample.weights.mean()/sherpa_weighted_sample.weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeqqg = ee_qq_ng(1, 100., 5., .3)\n",
    "rambo_mapping = RamboOnDiet(100., 3)\n",
    "mapped = MappedDensity(eeqqg, rambo_mapping)\n",
    "training_sample = import_hepmc('../samples/qcd/2-3/training.hepmc')\n",
    "training_sample = Sample(data=rambo_mapping.map_inverse(training_sample.data), weights=training_sample.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(training_sample.data)\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nice = Nice(training_sample, train_iters=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_sampler = ImportanceMC(mapped, nice)\n",
    "%time nice_sample = importance_sampler.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(nice_sample.weights[nice_sample.weights < 5000], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_sample = Sample(data=rambo_mapping.map(nice_sample.data), weights=nice_sample.weights)\n",
    "export_hepmc(100., mapped_sample, \"../samples/qcd/2-3/realnvp.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = nice_sample.weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_sample.weights.mean()/2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound=2500\n",
    "print('M:', bound)\n",
    "sample = importance_sampler.sample(1)\n",
    "x = sample.data\n",
    "print('x:', x)\n",
    "weight = sample.weights\n",
    "print('weight:', weight)\n",
    "rand = np.random.rand(1)\n",
    "print('u:', rand)\n",
    "sampling_pdf = nice.sess.run(nice.dist.prob(x.astype(np.float32)))\n",
    "print('g(x):', sampling_pdf)\n",
    "target_pdf = mapped.pdf(x)\n",
    "print('f(x):', target_pdf)\n",
    "print('f(x)/(M*g(x)):', target_pdf/(bound*sampling_pdf))\n",
    "print('weight/M:', weight/bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = AcceptRejectSampler(target=mapped, bound=bound, sampler=importance_sampler, sampling_pdf=nice.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sample = sampler.sample(10000, expected_efficiency=.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcceptRejectSampler(object):\n",
    "    \"\"\" Acceptance Rejection method for sampling a given pdf.\n",
    "    \n",
    "    The method uses a known distribution and sampling method to propose\n",
    "    samples which are then accepted with the probability\n",
    "    pdf(x)/(c * sampling_pdf(x)), thus producing the desired distribution. \n",
    "    The resulting sample is unweighted.\n",
    "    \n",
    "    .. todo::\n",
    "        Handle points that lie above the bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, bound: float, \n",
    "            sampler = None, sampling_pdf = None) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        target\n",
    "            Unnormalized desired probability distribution of the sample.\n",
    "        bound\n",
    "            Constant such that pdf(x) <= bound * sampling_pdf(x)\n",
    "            for all x in the range of sampling.\n",
    "        sampler\n",
    "            The sampler which generates the sample. The default is a uniform sampler.\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.bound = bound\n",
    "        self.ndim = target.ndim\n",
    "\n",
    "        if sampler is None:\n",
    "            sampler = UniformSampler(target)\n",
    "            def sampling_pdf(x):\n",
    "                return np.ones(x.size)\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.sampling_pdf = sampling_pdf\n",
    "\n",
    "    def sample(self, sample_size: int, expected_efficiency: float = 1.) -> None:\n",
    "        x = np.empty((sample_size, self.ndim))\n",
    "\n",
    "        #indices = np.arange(sample_size)\n",
    "        n_todo = sample_size\n",
    "        trials = 0\n",
    "        while n_todo > 0:\n",
    "            print('n_todo:', n_todo)\n",
    "            trials += int(n_todo/expected_efficiency)\n",
    "            sample = self.sampler.sample(int(n_todo/expected_efficiency))\n",
    "            proposal = sample.data\n",
    "            #accept = np.random.rand(indices.size) * self.bound * self.sampling_pdf(proposal) <= self.target.pdf(proposal)\n",
    "            #accept = np.random.rand(indices.size) * self.bound <= sample.weights\n",
    "            #accept = np.random.rand(indices.size) < sample.weights / self.bound\n",
    "            u = np.random.rand(int(n_todo/expected_efficiency))\n",
    "            accept = u < sample.weights / self.bound\n",
    "            n_accept = accept.sum()\n",
    "            if n_accept <= n_todo:\n",
    "                x[sample_size-n_todo:sample_size-n_todo+n_accept] = proposal[accept]\n",
    "            else:\n",
    "                accepted = proposal[accept]\n",
    "                x[sample_size-n_todo:] = accepted[:n_todo]\n",
    "            n_todo -= n_accept\n",
    "            #x[indices[accept]] = proposal[accept]\n",
    "            #indices = indices[np.logical_not(accept)]\n",
    "        print('Unweighting eff.:', sample_size/trials)\n",
    "        return Sample(data=x, target=self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE=tf.float32\n",
    "NP_DTYPE=np.float32\n",
    "USE_BATCHNORM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_bijectors = 5\n",
    "train_iters = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeqqg = ee_qq_ng(1, 100., 5., .3)\n",
    "rambo_mapping = RamboOnDiet(100., 3)\n",
    "mapped = MappedDensity(eeqqg, rambo_mapping)\n",
    "#met = DefaultMetropolis(mapped, Gaussian(5, .01))\n",
    "#start = met.sample(5000, np.random.rand(5)).data[-1] # warmup\n",
    "#%time met_sample = met.sample(10000, start)\n",
    "training_sample = import_hepmc('../samples/qcd/2-3/training.hepmc')\n",
    "X = rambo_mapping.map_inverse(training_sample.data)\n",
    "#X = np.hstack((X, np.ones((X.shape[0], 1)))) # make ndim even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X.astype(NP_DTYPE))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size=X.shape[0])\n",
    "dataset = dataset.prefetch(3 * batch_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "data_iterator = dataset.make_one_shot_iterator()\n",
    "x_samples = data_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(tfb.Bijector):\n",
    "    def __init__(self, eps=1e-5, decay=0.95, validate_args=False, name=\"batch_norm\"):\n",
    "        super(BatchNorm, self).__init__(\n",
    "            event_ndims=1, validate_args=validate_args, name=name)\n",
    "        self._vars_created = False\n",
    "        self.eps = eps\n",
    "        self.decay = decay\n",
    "\n",
    "    def _create_vars(self, x):\n",
    "        n = x.get_shape().as_list()[1]\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.beta = tf.get_variable('beta', [1, n], dtype=DTYPE)\n",
    "            self.gamma = tf.get_variable('gamma', [1, n], dtype=DTYPE)\n",
    "            self.train_m = tf.get_variable(\n",
    "                'mean', [1, n], dtype=DTYPE, trainable=False)\n",
    "            self.train_v = tf.get_variable(\n",
    "                'var', [1, n], dtype=DTYPE, initializer=tf.ones_initializer, trainable=False)\n",
    "        self._vars_created = True\n",
    "\n",
    "    def _forward(self, u):\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(u)\n",
    "        return (u - self.beta) * tf.exp(-self.gamma) * tf.sqrt(self.train_v + self.eps) + self.train_m\n",
    "\n",
    "    def _inverse(self, x):\n",
    "        # Eq 22. Called during training of a normalizing flow.\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(x)\n",
    "        # statistics of current minibatch\n",
    "        m, v = tf.nn.moments(x, axes=[0], keep_dims=True)\n",
    "        # update train statistics via exponential moving average\n",
    "        update_train_m = tf.assign_sub(\n",
    "            self.train_m, self.decay * (self.train_m - m))\n",
    "        update_train_v = tf.assign_sub(\n",
    "            self.train_v, self.decay * (self.train_v - v))\n",
    "        # normalize using current minibatch statistics, followed by BN scale and shift\n",
    "        with tf.control_dependencies([update_train_m, update_train_v]):\n",
    "            return (x - m) * 1. / tf.sqrt(v + self.eps) * tf.exp(self.gamma) + self.beta\n",
    "\n",
    "    def _inverse_log_det_jacobian(self, x):\n",
    "        # at training time, the log_det_jacobian is computed from statistics of the\n",
    "        # current minibatch.\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(x)\n",
    "        _, v = tf.nn.moments(x, axes=[0], keep_dims=True)\n",
    "        abs_log_det_J_inv = tf.reduce_sum(\n",
    "            self.gamma - .5 * tf.log(v + self.eps))\n",
    "        return abs_log_det_J_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros([5], DTYPE))\n",
    "#uniform = tfd.Uniform(low=tf.zeros([5], DTYPE), high=tf.ones([5], DTYPE))\n",
    "#base_dist = tfd.Independent(\n",
    "#    distribution=tfd.Uniform(low=tf.zeros([5], DTYPE), high=tf.ones([5], DTYPE)), \n",
    "#    reinterpreted_batch_ndims=1)\n",
    "#base_dist = tfd.Independent(\n",
    "#    distribution=tfd.TruncatedNormal(loc=tf.zeros([5], DTYPE), scale=1., low=tf.zeros([5], DTYPE), high=tf.ones([5], DTYPE)),\n",
    "#    reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.sample(2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.prob(base_dist.sample(10)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors = []\n",
    "\n",
    "for i in range(num_bijectors):\n",
    "    #bijectors.append(NVPCoupling(D=2, d=1, layer_id=i))\n",
    "    bijectors.append(tfb.RealNVP(num_masked=3, shift_and_log_scale_fn=tfb.real_nvp_default_template(hidden_layers=[512, 512])))\n",
    "    if USE_BATCHNORM and i % 2 == 0:\n",
    "        # BatchNorm helps to stabilize deep normalizing flows, esp. Real-NVP\n",
    "        bijectors.append(BatchNorm(name='batch_norm%d' % i))\n",
    "    #if i % 2 == 0:\n",
    "    #    bijectors.append(tfb.Permute(permutation=[2, 3, 4, 0, 1]))\n",
    "    #else:\n",
    "    #    bijectors.append(tfb.Permute(permutation=[3, 4, 0, 1, 2]))\n",
    "    #bijectors.append(tfb.Permute(permutation=[3, 4, 2, 0, 1]))\n",
    "    bijectors.append(tfb.Permute(permutation=[2, 3, 4, 0, 1]))\n",
    "# Discard the last Permute layer.\n",
    "flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=flow_bijector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "x = base_dist.sample(8000)\n",
    "samples = [x]\n",
    "names = [base_dist.name]\n",
    "for bijector in reversed(dist.bijector.bijectors):\n",
    "    x = bijector.forward(x)\n",
    "    samples.append(x)\n",
    "    names.append(bijector.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sess.run(samples)\n",
    "f, arr = plt.subplots(1, len(results), figsize=(4 * (len(results)), 4))\n",
    "X0 = results[0]\n",
    "for i in range(len(results)):\n",
    "    X1 = results[i]\n",
    "    idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] < 0)\n",
    "    arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='red')\n",
    "    idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] < 0)\n",
    "    arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='green')\n",
    "    idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] > 0)\n",
    "    arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='blue')\n",
    "    idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] > 0)\n",
    "    arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='black')\n",
    "    arr[i].set_xlim([-10, 10])\n",
    "    arr[i].set_ylim([-10, 10])\n",
    "    arr[i].set_title(names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(dist.log_prob(x_samples))\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_STEPS = int(train_iters)\n",
    "global_step = []\n",
    "np_losses = []\n",
    "for i in range(NUM_STEPS):\n",
    "    _, np_loss = sess.run([train_op, loss])\n",
    "    if i % 1000 == 0:\n",
    "        global_step.append(i)\n",
    "        np_losses.append(np_loss)\n",
    "    #if i % int(1e4) == 0:\n",
    "        print(i, np_loss)\n",
    "start = 0\n",
    "plt.plot(np_losses[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "x = base_dist.sample(8000)\n",
    "samples = [x]\n",
    "names = [base_dist.name]\n",
    "for bijector in reversed(dist.bijector.bijectors):\n",
    "    x = bijector.forward(x)\n",
    "    samples.append(x)\n",
    "    names.append(bijector.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sess.run(samples)\n",
    "#X0 = results[0]\n",
    "#rows = 2\n",
    "#cols = int(len(results) / 2)\n",
    "#f, arr = plt.subplots(2, cols, figsize=(4 * (cols), 4 * rows))\n",
    "#i = 0\n",
    "#for r in range(rows):\n",
    "#    for c in range(cols):\n",
    "#        X1 = results[i]\n",
    "#        idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] < 0)\n",
    "#        arr[r, c].scatter(X1[idx, 0], X1[idx, 1], s=10, color='red')\n",
    "#        idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] < 0)\n",
    "#        arr[r, c].scatter(X1[idx, 0], X1[idx, 1], s=10, color='green')\n",
    "#        idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] > 0)\n",
    "#        arr[r, c].scatter(X1[idx, 0], X1[idx, 1], s=10, color='blue')\n",
    "#        idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] > 0)\n",
    "#        arr[r, c].scatter(X1[idx, 0], X1[idx, 1], s=10, color='black')\n",
    "#        arr[r, c].set_xlim([-5, 5])\n",
    "#        arr[r, c].set_ylim([-5, 5])\n",
    "#        arr[r, c].set_title(names[i])\n",
    "#\n",
    "#        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[0])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[1])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[2])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[3])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[4])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[5])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[6])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[7])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[8])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[9])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15, range=[[0, 1], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[10])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[11])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[12])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[13])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[14])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(results[15])\n",
    "f = sns.PairGrid(data)\n",
    "f = f.map_diag(plt.hist, bins=15)\n",
    "f = f.map_offdiag(plt.hist2d, bins=15, range=[[0, 1], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(target, eval_count):\n",
    "    xs = np.empty((eval_count, 5), dtype=np.float32)\n",
    "    ys = np.empty(eval_count, dtype=np.float32)\n",
    "    weights = np.empty(eval_count)\n",
    "    trials = 0\n",
    "\n",
    "    indices = np.arange(eval_count)\n",
    "    while indices.size > 0:\n",
    "        trials += indices.size\n",
    "        x = base_dist.sample(indices.size)\n",
    "        samples = [x]\n",
    "        for bijector in reversed(dist.bijector.bijectors):\n",
    "            x = bijector.forward(x)\n",
    "            samples.append(x)\n",
    "        results = sess.run(samples)\n",
    "        x = results[-1]\n",
    "        #x = self.dist.rvs(indices.size)\n",
    "        y = target.pdf(x)\n",
    "        in_bounds = y != 0.\n",
    "        xs[indices[in_bounds]] = x[in_bounds]\n",
    "        ys[indices[in_bounds]] = y[in_bounds]\n",
    "        indices = indices[np.logical_not(in_bounds)]\n",
    "\n",
    "    #weights = ys / self.dist.pdf(xs)\n",
    "    weights = ys / dist.prob(xs).eval()\n",
    "    integral = eval_count/trials * weights.mean()\n",
    "    stderr = np.sqrt(weights.var() * eval_count/trials**2)\n",
    "    sample = Sample(data=xs, target=target, pdf=ys, weights=weights)\n",
    "\n",
    "    return sample, integral, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, integral, err = integrate(mapped, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_sample = Sample(data=rambo_mapping.map(sample.data), weights=sample.weights)\n",
    "export_hepmc(100., mapped_sample, \"../samples/qcd/2-3/realnvp.hepmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = sample.weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = AcceptRejectSampler(target=eeqqg, bound=bound, sampler=importance_sampler, sampling_pdf=sarge.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
