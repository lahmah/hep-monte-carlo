{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hepmc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionBasis(object):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def output_matrix(self, xs, params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def random_node_params(self, node_count):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def gradient(self, out_weights, node_params, xs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def output(self, out_weights, node_params, xs):\n",
    "        \"\"\"\n",
    "        :param xs: Numpy array, positions to evaluate function at.\n",
    "        :return: The approximated function value at the given positions.\n",
    "        \"\"\"\n",
    "        xs = util.interpret_array(xs, self.in_dim)\n",
    "\n",
    "        out = np.dot(self.output_matrix(xs, node_params), out_weights)\n",
    "        return out\n",
    "    \n",
    "    def output_vectorial(self, out_weights, node_params, *xs):\n",
    "        if np.isscalar(xs[0]):\n",
    "            # xs are numbers\n",
    "            xs = np.stack(xs, axis=0)\n",
    "            return self.output(out_weights, node_params, xs)\n",
    "        else:\n",
    "            shape = np.asanyarray(xs[0]).shape\n",
    "            xs = np.stack([np.asanyarray(x).flatten() for x in xs], axis=1)\n",
    "            res = self.output(out_weights, node_params, xs)\n",
    "            return res.reshape(shape)\n",
    "    \n",
    "    def gradient_vectorial(self, out_weights, node_params, *xs):\n",
    "        if np.isscalar(xs[0]):\n",
    "            # xs are numbers\n",
    "            xs = np.stack(xs, axis=0)\n",
    "            return self.output(out_weights, node_params, xs)\n",
    "        else:\n",
    "            shape = np.asanyarray(xs[0]).shape\n",
    "            xs = np.stack([np.asanyarray(x).flatten() for x in xs], axis=1)\n",
    "            res = self.gradient(out_weights, node_params, xs)\n",
    "            return res.reshape((*shape, self.in_dim))\n",
    "\n",
    "\n",
    "class AdditiveBasis(FunctionBasis):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1, weight_range=(-1, 1), bias_range=(0, -1)):\n",
    "        \"\"\" Linear combination of input followed by non-linear function.\n",
    "\n",
    "        This class uses the model for additive activation functions from\n",
    "        \"Hamiltonian Monte Carlo acceleration using surrogate\n",
    "        functions with random bases\" (ArXiv ID: 1506.05555):\n",
    "\n",
    "        If the input is xj and the input weights are wi with biases bi\n",
    "        (the index i corresponds to the i-th node), the output is\n",
    "        z(xj) = sum_i gi(wi * xj + bi),\n",
    "        where the wi and xj are all ndim-dimensional and bi are scalars,\n",
    "        gi are the activation functions.\n",
    "\n",
    "\n",
    "        Note that the behaviour is different from Radial Base functions, since\n",
    "        the position of the \"centers\" do not only depend on the biases.\n",
    "        The weights do scale the base function, however it is only applied\n",
    "        linearly in one dimension.\n",
    "\n",
    "        Assuming all gi = g are the same and g has a center at 0 (as they would\n",
    "        be for radial base functions). Then the center for node i would be at\n",
    "        bi / wi. This behavior can be problematic if g is radial/centered,\n",
    "        as the biases can not be chosen just based on the x-space region.\n",
    "        Additionally the bounds for the input biases could not be chosen just\n",
    "        based on the x-space, thus additive base functions are not a substitute\n",
    "        for radial base functions.\n",
    "\n",
    "        :param dim: Dimensionality of variable space (value space is 1D)\n",
    "        :param weight_range: Range the input weight can have.\n",
    "        :param bias_range: Range the input bias can have.\n",
    "        \"\"\"\n",
    "        super().__init__(in_dim, out_dim)\n",
    "\n",
    "        self.weight_range = weight_range\n",
    "        self.bias_range = bias_range\n",
    "\n",
    "    def basis_function(self, inputs, fn_params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def basis_function_gradient(self, inputs, fn_params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def random_fn_params(self, node_count):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def output_matrix(self, xs, params, fn=None):\n",
    "        biases, in_weights, fn_params = params\n",
    "        if fn is None:\n",
    "            fn = self.basis_function\n",
    "\n",
    "        # inputs: node_count * in_dim\n",
    "        inputs = biases[np.newaxis, :] + np.dot(xs, in_weights.transpose())\n",
    "\n",
    "        outputs = fn(inputs, fn_params)\n",
    "        return outputs\n",
    "\n",
    "    def random_node_params(self, node_count, out_bias=False):\n",
    "        biases = np.random.uniform(*self.bias_range, node_count)\n",
    "\n",
    "        input_weights = np.random.uniform(*self.weight_range, (node_count, self.in_dim))\n",
    "        if out_bias: \n",
    "            # basis function bust be != 0 at x = 0\n",
    "            input_weights[0] = 0\n",
    "            \n",
    "        return biases, input_weights, self.random_fn_params(node_count)\n",
    "\n",
    "    def gradient(self, out_weights, params, xs):\n",
    "        xs = util.interpret_array(xs, self.in_dim)\n",
    "\n",
    "        out_matrix = self.output_matrix(xs, params, self.basis_function_gradient)\n",
    "        # n: sample, m: out_dimension, k: in_dimension\n",
    "        out = np.einsum('ni,im,ik->nmk', out_matrix, out_weights, params[1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class RadialBasis(FunctionBasis):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1, width_range=(.01, 1), center_range=(0, 1)):\n",
    "        \"\"\" Linear combination of input followed by non-linear function.\n",
    "\n",
    "        For a given number of nodes N, use N ndim-dimensional centers ci\n",
    "        (s.t. -ci is the bias) and N widths wi (1-dim) to approximate\n",
    "        the output to xi input points:\n",
    "        z(xi) = sum_i gi( | (xi + bi) / (2wi) |)\n",
    "        where gi are activation functions taking 1D inputs.\n",
    "\n",
    "        :param in_dim: Dimensionality of input space.\n",
    "        :param out_dim: Dimensionality of output space.\n",
    "        :param width_range: Range the widths can be in. Tuple of either scalars\n",
    "            or in_dim-dimensional numpy arrays (if multi_widths is True).\n",
    "        :param center_range: Range the centers can be in. Choose such that\n",
    "            the centers span the x-space. Tuple of either scalars or\n",
    "            in_dim-dimensional numpy arrays.\n",
    "        \"\"\"\n",
    "        super().__init__(in_dim, out_dim)\n",
    "\n",
    "        self.weight_range = width_range\n",
    "        self.bias_range = center_range\n",
    "\n",
    "    def basis_function(self, inputs, fn_params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def basis_function_gradient(self, inputs, fn_params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def random_fn_params(self, node_count):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def output_matrix(self, xs, params, fn=None):\n",
    "        xs = util.interpret_array(xs, self.in_dim)\n",
    "        centers, widths, fn_params = params\n",
    "        if fn is None:\n",
    "            fn = self.basis_function\n",
    "\n",
    "        # inputs: xs.size * node_count * ndim\n",
    "        inputs = xs[:, np.newaxis, :] - centers[np.newaxis, :, :]\n",
    "        inputs = (np.linalg.norm(inputs, axis=2) / widths)**2 / 2\n",
    "\n",
    "        outputs = fn(inputs, fn_params)\n",
    "        return outputs\n",
    "\n",
    "    def random_node_params(self, node_count, out_bias=False):\n",
    "        centers = np.random.uniform(*self.bias_range, (node_count, self.in_dim))\n",
    "\n",
    "        widths = np.random.uniform(*self.weight_range, node_count)\n",
    "        if out_bias:\n",
    "            widths[0] = np.inf\n",
    "            centers[0] = 0\n",
    "\n",
    "        return centers, widths, self.random_fn_params(node_count)\n",
    "\n",
    "    def gradient(self, out_weights, params, xs):\n",
    "        xs = util.interpret_array(xs, self.in_dim)\n",
    "        xs = np.array(xs, copy=False, subok=True, ndmin=2)\n",
    "        centers, widths, fn_params = params\n",
    "\n",
    "        out_mat = self.output_matrix(xs, params, self.basis_function_gradient)\n",
    "        # j: data index, i: node index, k: in-dimension index, m: out-dimension\n",
    "        out = np.einsum('ji,im,jik->jmk', out_mat, out_weights / widths ** 2,\n",
    "                        (xs[:, np.newaxis, :] - centers[np.newaxis, :, :]))\n",
    "        return out\n",
    "\n",
    "\n",
    "# CONCRETE ADDITIVE BASES\n",
    "class TrigBasis(AdditiveBasis):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1, weight_range=(0, 1), bias_range=(-1, 0)):\n",
    "        \"\"\" Additive function basis using a single Gaussian as non-linearity.\n",
    "\n",
    "        :param in_dim: Dimensionality of input space.\n",
    "        :param out_dim: Dimensionality of output space.\n",
    "        :param weight_range: Range the weight can have.\n",
    "        :param bias_range: Range the input bias can have.\n",
    "        \"\"\"\n",
    "        super().__init__(in_dim, out_dim, weight_range, bias_range)\n",
    "\n",
    "    def basis_function(self, inputs, fn_params):\n",
    "        return np.cos(inputs)\n",
    "\n",
    "    def basis_function_gradient(self, inputs, fn_params):\n",
    "        return -np.sin(inputs)\n",
    "\n",
    "    def random_fn_params(self, node_count):\n",
    "        return None\n",
    "\n",
    "\n",
    "# CONCRETE RADIAL BASES\n",
    "class GaussianBasis(RadialBasis):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=1, width_range=(.01, 1), center_range=(0, 1)):\n",
    "        \"\"\" Radial function basis using a single Gaussian as non-linearity.\n",
    "\n",
    "        :param in_dim: Dimensionality of input space.\n",
    "        :param out_dim: Dimensionality of output space.\n",
    "        :param width_range: Range the widths can be in. Tuple of either scalars\n",
    "            or ndim-dimensional numpy arrays (if multi_widths is True).\n",
    "        :param center_range: Range the centers can be in. Choose such that\n",
    "            the centers span the x-space. Tuple of either scalars or\n",
    "            ndim-dimensional numpy arrays.\n",
    "        :param multi_widths: True if the widths are ndim-dimensional, i.e.\n",
    "            if the radial functions are stretched independently in each ndim.\n",
    "        \"\"\"\n",
    "        super().__init__(in_dim, out_dim, width_range, center_range)\n",
    "\n",
    "    def basis_function(self, inputs, fn_params):\n",
    "        return np.exp(-inputs)\n",
    "\n",
    "    def basis_function_gradient(self, inputs, fn_params):\n",
    "        return -np.exp(-inputs)\n",
    "\n",
    "    def random_fn_params(self, node_count):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSELMachine(object):\n",
    "    def __init__(self, node_count, fn_basis, node_params=None):\n",
    "        self.node_count = node_count\n",
    "        self.fn_basis = fn_basis\n",
    "        \n",
    "        if node_params is None:\n",
    "            node_params = fn_basis.random_node_params(node_count)\n",
    "        \n",
    "        self.node_params = node_params\n",
    "        self.out_weights = None\n",
    "        self.pinv = None\n",
    "        \n",
    "    def output(self, xs):\n",
    "        \"\"\"\n",
    "        :param xs: Numpy array, positions to evaluate function at.\n",
    "        :return: The approximated function value at the given positions.\n",
    "        \"\"\"\n",
    "        if self.out_weights is None:\n",
    "            raise RuntimeError(\"Cannot evaluate ELM before learning phase.\")\n",
    "        return self.fn_basis.output(self.out_weights, self.node_params, xs)    \n",
    "\n",
    "    def output_vectorial(self, *xs):\n",
    "        if self.out_weights is None:\n",
    "            raise RuntimeError(\"Cannot evaluate ELM before learning phase.\")\n",
    "        return self.fn_basis.output_vectorial(self.out_weights, self.node_params, *xs)    \n",
    "    \n",
    "    def gradient(self, xs):\n",
    "        if self.out_weights is None:\n",
    "            raise RuntimeError(\"Cannot evaluate ELM before learning phase.\")\n",
    "        return self.fn_basis.gradient(self.out_weights, self.node_params, xs)    \n",
    "\n",
    "    def gradient_vectorial(self, *xs):\n",
    "        if self.out_weights is None:\n",
    "            raise RuntimeError(\"Cannot evaluate ELM before learning phase.\")\n",
    "        return self.fn_basis.gradient_vectorial(self.out_weights, self.node_params, *xs)    \n",
    "    \n",
    "    def learn(self, xs, values):\n",
    "        xs = util.interpret_array(xs, self.fn_basis.in_dim)\n",
    "        values = util.interpret_array(values, self.fn_basis.out_dim)\n",
    "\n",
    "        if self.out_weights is None:\n",
    "            \n",
    "            if xs.shape[0] < self.node_count:\n",
    "                raise RuntimeWarning(\"To ensure the output matrix has full rank, \"\n",
    "                                     \"the initial dataset should have at least \"\n",
    "                                     \"as many samples as there are nodes.\")\n",
    "            out_matrix = self.fn_basis.output_matrix(xs, self.node_params)\n",
    "            \n",
    "#             # OLD VERSION\n",
    "            pinv = np.linalg.pinv(out_matrix)\n",
    "            weights = np.dot(pinv, values)\n",
    "            self.out_weights = weights\n",
    "            self.pinv = np.linalg.inv(out_matrix.transpose().dot(out_matrix))\n",
    "            \n",
    "#             self.pinv = np.dot(pinv, np.linalg.pinv(out_matrix.transpose()))\n",
    "            return self.out_weights, self.pinv\n",
    "        \n",
    "            # NEW VERSION\n",
    "            # initial learning sequence, standard ELM\n",
    "            self.pinv = np.linalg.inv(out_matrix.transpose().dot(out_matrix))\n",
    "\n",
    "            self.out_weights = np.dot(self.pinv.dot(out_matrix.transpose()), values)\n",
    "            return self.out_weights, self.pinv\n",
    "        \n",
    "        out_matrix = self.fn_basis.output_matrix(xs, self.node_params)\n",
    "        differences = values - np.dot(out_matrix, self.out_weights)\n",
    "        if xs.shape[0] == 1:\n",
    "            # single point update\n",
    "            self.pinv -= np.einsum('ij,kj,kl,lm', self.pinv, out_matrix, out_matrix, self.pinv) / (\n",
    "                         1 + np.einsum('ij,jk,lk', out_matrix, self.pinv, out_matrix))\n",
    "            self.out_weights += np.einsum('ij,kj,kl', self.pinv, out_matrix, differences)\n",
    "            return self.out_weights, self.pinv\n",
    "    \n",
    "#         # iterative batch update\n",
    "#         svd = np.linalg.pinv(1 + np.einsum('ij,jk,lk', out_matrix, self.pinv, out_matrix))\n",
    "#         self.pinv -= np.einsum('ij,kj,kl,lm,mn', self.pinv, out_matrix, svd, out_matrix, self.pinv)\n",
    "#         self.out_weights += np.einsum('ij,kj,kl', self.pinv, out_matrix, differences)\n",
    "#         return self.out_weights, self.pinv\n",
    "        H = self.fn_basis.output_matrix(xs, self.node_params)\n",
    "        HT = H.transpose()\n",
    "        HTH = np.dot(HT, H)\n",
    "        batch_size = xs.shape[0]\n",
    "        I = np.eye(batch_size)\n",
    "        Hp = np.dot(H, self.pinv)\n",
    "        HpHT = np.dot(Hp, HT)\n",
    "        temp = np.linalg.inv(I + HpHT)\n",
    "        pHT = np.dot(self.pinv, HT)\n",
    "        self.pinv = p = self.pinv - np.dot(np.dot(pHT, temp), Hp)\n",
    "        pHT = np.dot(p, HT)\n",
    "        Hbeta = np.dot(H, self.out_weights)\n",
    "        self.out_weights = out_weights = self.out_weights + np.dot(pHT, values - Hbeta)\n",
    "        return out_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis = TrigBasis(1, 1, (0, 100), (-1, 0))\n",
    "basis = GaussianBasis(1, center_range=(0, 2), width_range=(0.01, 1))\n",
    "\n",
    "N = 81\n",
    "node_count = 80\n",
    "split = 80\n",
    "\n",
    "fn = lambda xs: np.sin(40 * xs)*np.sin(10*xs)\n",
    "xs = np.random.rand(N).astype(np.double).reshape(N, 1)*2\n",
    "values = fn(xs)\n",
    "\n",
    "elm = OSELMachine(node_count, basis)\n",
    "elm.learn(xs[:split], values[:split])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "x = np.linspace(0, 1, 1000) * 2\n",
    "plt.plot(x, fn(x), label=\"function\")\n",
    "plt.plot(xs[split:], values[split:], 'x')\n",
    "plt.plot(x, elm.output(x), label=\"initial\")\n",
    "# plt.plot(x, elm.gradient_vectorial(x)/100, label=\"gradient\")\n",
    "# plt.plot(-params[0], [-1.5]*node_count, '+')\n",
    "\n",
    "# elm.learn(xs[split:], values[split:])\n",
    "for i in range(node_count, N):\n",
    "    elm.learn(xs[i], values[i])\n",
    "\n",
    "plt.plot(x, elm.output(x), label=\"final\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_matrix = elm.fn_basis.output_matrix(xs[split:], elm.node_params)\n",
    "differences = values[split:] - np.dot(out_matrix, elm.out_weights)\n",
    "inn = 1 + np.einsum('ij,jk,lk', out_matrix, elm.pinv, out_matrix)\n",
    "u, s, vh = np.linalg.svd(inn, full_matrices=True)\n",
    "svd = vh.transpose().dot(np.diag(1/s).dot(u.transpose()))\n",
    "# svd = np.linalg.pinv(inn)\n",
    "newpinv = elm.pinv - np.einsum('ij,kj,kl,lm,mn', elm.pinv, out_matrix, svd, out_matrix, elm.pinv)\n",
    "dow = np.einsum('ij,kj,kl', elm.pinv, out_matrix, differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elm.out_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpinv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(inn, np.linalg.inv(inn)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_matrix = elm.fn_basis.output_matrix(xs[:80], elm.node_params)\n",
    "inns = out_matrix.transpose().dot(out_matrix)\n",
    "pinv = np.linalg.pinv(inns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.dot(inns, pinv), np.eye(80), atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(object):\n",
    "    __slots__ = ['foo', 'bar']\n",
    "    def __init__(self):\n",
    "        self.foo = 3\n",
    "    def __repr__(self):\n",
    "        return str(self.foo)\n",
    "    \n",
    "class Bar(Foo):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Foo()\n",
    "b = Bar(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cm = 100.\n",
    "mapping = phase_space.RamboOnDiet(e_cm, 2)\n",
    "target = phase_space.MappedDensity(densities.ee_qq(e_cm), mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sample = PlainMC(2)(target, 50000)\n",
    "int_sample.integral, int_sample.integral_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.norm = int_sample.integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100) \n",
    "mgrid = np.meshgrid(x, y)\n",
    "prob = target(*mgrid)\n",
    "plt.imshow(prob, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DefaultMetropolis(2, target)\n",
    "sample =sampler.sample(1000, [.5, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 50\n",
    "nlearn = 1000\n",
    "x = np.linspace(-1, 2, 100)\n",
    "y = np.linspace(-1, 2, 100)\n",
    "mgrid = np.meshgrid(x, y)\n",
    "\n",
    "xvals = np.random.random((nlearn, 2))\n",
    "fvals = target.pdf(xvals)\n",
    "\n",
    "# surrogate\n",
    "basis = surrogate.extreme_learning.GaussianBasis(2)\n",
    "log_vals = -np.ma.log(fvals)\n",
    "xvals = xvals[~log_vals.mask]\n",
    "log_vals = log_vals[~log_vals.mask]\n",
    "# train\n",
    "params = basis.extreme_learning_train(xvals, log_vals, nodes)\n",
    "\n",
    "# surrogate gradient\n",
    "def surrogate_gradient(xs):\n",
    "    return basis.eval_gradient(*params, xs)\n",
    "target.pot_gradient = surrogate_gradient\n",
    "util.count_calls(target, 'pot_gradient')\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(basis.eval_split(*params, *mgrid))\n",
    "plt.subplot(122)\n",
    "plt.imshow(-np.log(target(*mgrid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = hamiltonian.StaticSphericalHMC(target, .03, .03, 30, 30)\n",
    "# (target, 2, .5, 1., 30, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sampler.sample(1000, initial, log_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(*sample.data.transpose(), 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
